{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f1f800c-9004-4662-a84e-6b9e58cdc9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#!pip install wikipedia\n",
    "#!pip install yake\n",
    "#!pip install --upgrade ecommercetools\n",
    "#!pip install pattern\n",
    "#!pip install textacy\n",
    "\n",
    "# Import packages\n",
    "import wikipedia\n",
    "import re\n",
    "import yake\n",
    "import nltk #For some reason, had to uninstall and reinstall\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ecommercetools import seo\n",
    "#from pattern.text.en import singularize, pluralize #issues here\n",
    "#For Google Knowledge Graph API\n",
    "import requests\n",
    "import urllib\n",
    "import json\n",
    "import os\n",
    "from pattern.text.en import singularize, pluralize\n",
    "from requests_html import HTML\n",
    "from requests_html import HTMLSession\n",
    "#if scraping paragraphs from first few webpages\n",
    "from bs4 import BeautifulSoup\n",
    "#For question generation\n",
    "import spacy\n",
    "import textacy\n",
    "#Two lines below only need run the first time\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('omw-1.4'\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "pd.set_option('max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a01ed597-7e02-4ed4-a8a1-1d13079471de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ending_pluralize(noun):\n",
    "    '''Return most appropriate plural of the input word.'''\n",
    "    if re.search('[sxz]$', noun):\n",
    "        return re.sub('$', 'es', noun)\n",
    "    elif re.search('[^aeioudgkprt]h$', noun):\n",
    "        return re.sub('$', 'es', noun)\n",
    "    elif re.search('y$', noun):\n",
    "        return re.sub('y$', 'ies', noun)\n",
    "    else:\n",
    "        return noun\n",
    "\n",
    "def add_s_pluralize(noun):\n",
    "    '''Naively add s to end of input word to create plural'''\n",
    "    return noun + 's'\n",
    "  \n",
    "def tidy_input(input):\n",
    "    '''Take input word and tidy it up to create a list of options.\n",
    "    \n",
    "    We have a few different pluralize functions just to account for any\n",
    "    misspellings online/words created when punctuation removed.\n",
    "    '''\n",
    "  \n",
    "    input_words = input.split()\n",
    "  \n",
    "    #Add singular forms of plurals and plural forms of singles \n",
    "    singles = [singularize(plural) for plural in input_words]\n",
    "    plurals1 = [pluralize(single) for single in singles]\n",
    "    plurals2 = [ending_pluralize(single) for single in singles]\n",
    "    plurals3 = [add_s_pluralize(single) for single in singles]\n",
    "    input_words = input_words + singles + plurals1 + plurals2 + plurals3\n",
    "  \n",
    "    input_words = input_words + [word.lower() for word in input_words]\n",
    "    #If you want capitalized words as well\n",
    "    input_words = input_words + [word[0].upper() + word[1:] for word in input.split()]\n",
    "    input_words = input_words + [word.upper() for word in input_words]\n",
    "  \n",
    "    input_words = list(set(input_words))\n",
    "    \n",
    "    return(input_words)\n",
    "  \n",
    "#Next few functions sourced from https://practicaldatascience.co.uk/data-science/how-to-access-the-google-knowledge-graph-search-api\n",
    "  \n",
    "def get_source(url):\n",
    "    \"\"\"Returns the source code for the provided URL. \n",
    "  \n",
    "    Parameters\n",
    "      ----------\n",
    "    url (string): URL of the page to scrape.\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    response (object): HTTP response object from requests_html. \n",
    "    \"\"\"\n",
    "  \n",
    "    try:\n",
    "        session = HTMLSession()\n",
    "        response = session.get(url)\n",
    "        return response\n",
    "  \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "\n",
    "def get_knowledge_graph(api_key, query):\n",
    "    \"\"\"Return a Google Knowledge Graph for a given query.\n",
    "\n",
    "    Parameters\n",
    "    ---------- \n",
    "    api_key (string): Google Knowledge Graph API key. \n",
    "    query (string): Term to search for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    response (object): Knowledge Graph response object in JSON format.\n",
    "    \"\"\" \n",
    "\n",
    "    endpoint = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
    "    params = {\n",
    "      'query': query,\n",
    "      'limit': 10,\n",
    "      'indent': True,\n",
    "      'key': api_key,\n",
    "    }\n",
    "\n",
    "    url = endpoint + '?' + urllib.parse.urlencode(params)    \n",
    "    response = get_source(url)\n",
    "\n",
    "    return json.loads(response.text)\n",
    "  \n",
    "def get_knowledge_graph_df(input):\n",
    "    \"\"\"\n",
    "    Uses Google's knowledge graph to generate Pandas DataFrame of entities \n",
    "    deemed most similar to input searched. DataFrame includes categorization\n",
    "    of entity, title, short description and URL (usually to Wikipedia).\n",
    "    You will need to have set up an API key in Google Cloud Console to get this\n",
    "    to work (it's free to do and you can do 100k requests a day I believe.)\n",
    "    https://console.cloud.google.com/apis \n",
    "    Parameters\n",
    "    ----------\n",
    "    input (string): Final Linkee answer\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    knowledge_graph_df (Pandas DataFrame): info on Knowledge Graph results\n",
    "    \"\"\"\n",
    "    threshold=0.2\n",
    "    api_key = os.environ['GOOGLE_LINKEE_KEY']\n",
    "    knowledge_graph_json = get_knowledge_graph(api_key, input)\n",
    "    knowledge_graph_df = pd.json_normalize(knowledge_graph_json, record_path='itemListElement')\n",
    "    return knowledge_graph_df\n",
    "    #Only using scores if knowledge graph actually returns something\n",
    "    if len(knowledge_graph_df) > 0:\n",
    "        max_score = max(knowledge_graph_df['resultScore'])\n",
    "        knowledge_graph_df = knowledge_graph_df.loc[knowledge_graph_df['resultScore']>threshold*max_score]\n",
    "        index_match = knowledge_graph_df.index[knowledge_graph_df['result.name'] == input]\n",
    "        if len(index_match) == 1:\n",
    "            n = index_match[0]\n",
    "            knowledge_graph_df = pd.concat([knowledge_graph_df.iloc[[n],:], knowledge_graph_df.drop(n, axis=0)], axis=0)\n",
    "            knowledge_graph_df.reset_index(inplace = True, drop = True )\n",
    "    return knowledge_graph_df\n",
    "  \n",
    "def classify_input(knowledge_graph_df):\n",
    "    \"\"\"Classify the input word/phrase as a certain category \n",
    "    to improve search results. Acts as failsafe if initial search\n",
    "    of input fails.\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    knowledge_graph_df: Return of get_knowledge_graph_df\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    category (string): Category of input\n",
    "    \n",
    "    \"\"\"\n",
    "    if \"SportsTeam\" in knowledge_graph_df['result.@type'][0]:\n",
    "        entity_tags = knowledge_graph_df['result.@type'][1]\n",
    "    else:\n",
    "        entity_tags = knowledge_graph_df['result.@type'][0]\n",
    "    \n",
    "    if (\"Movie\" in entity_tags) or (\"MovieSeries\" in entity_tags):\n",
    "        category = \"Movie\"\n",
    "    elif (\"TVEpisode\" in entity_tags) or (\"TVSeries\" in entity_tags):\n",
    "        category = \"TV\"\n",
    "    elif (\"VideoGame\" in entity_tags) or (\"VideoGameSeries\" in entity_tags):\n",
    "        category = \"VideoGame\"\n",
    "    elif (\"Book\" in entity_tags) or (\"BookSeries\" in entity_tags):\n",
    "        category = \"Book\"\n",
    "    elif \"Person\" in entity_tags:\n",
    "        category = \"Person\"\n",
    "    elif (\"MusicAlbum\" in entity_tags) or (\"MusicGroup\" in entity_tags) or (\"MusicRecording\" in entity_tags):\n",
    "        category = \"Music\"\n",
    "    elif (\"Place\" in entity_tags) or (\"AdministrativeArea\" in entity_tags):\n",
    "        category = \"Place\" \n",
    "    else:\n",
    "        category = \"Thing\"\n",
    "  \n",
    "    return(category)\n",
    "  \n",
    "def tailored_search(category, input):\n",
    "    \"\"\"Change the search to get better keywords for input, based on its category\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    category (string): Category of input\n",
    "    input (string): Final Linkee answer\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    search_input (string): Search term to use to find keywords\n",
    "    \n",
    "    \"\"\"\n",
    "    if category == \"Movie\" or category == \"TV\" or category == \"Book\":\n",
    "        search_input = input + \" \" + category + \" information\"\n",
    "    elif category == \"Place\":\n",
    "        search_input = input + \" location\"\n",
    "    else:\n",
    "        search_input = input\n",
    "    return(search_input)\n",
    "  \n",
    "def collect_urls(knowledge_graph_df):\n",
    "    \"\"\"Collect the urls from the knowledge graph to give more options to scrape \n",
    "    from.\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    knowledge_graph_df: Return of get_knowledge_graph_df\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    list of urls (string): urls found\n",
    "    \n",
    "    \"\"\"\n",
    "    if 'result.detailedDescription.url' in knowledge_graph_df.columns:\n",
    "        knowledge_graph_df = knowledge_graph_df[knowledge_graph_df['result.detailedDescription.url'].notna()]\n",
    "        urlList = knowledge_graph_df['result.detailedDescription.url'].tolist()\n",
    "    else:\n",
    "        urlList = []\n",
    "    return urlList\n",
    "\n",
    "def get_wiki_links(urlList):\n",
    "    '''Extract the URLs linking to Wikipedia from a list of URLs'''\n",
    "    url_wiki=[urlList[i] for i in range(len(urlList)) if urlList[i].find(\"wiki\")!= -1]\n",
    "    # if len(url_wiki) == 0:\n",
    "    #   print('No Urls')\n",
    "    # if url_wiki:\n",
    "    return(url_wiki)\n",
    "  \n",
    "def get_wiki_text(url_wiki, keep_words=10000):\n",
    "    '''\n",
    "    Takes a list of urls and scrapes from Wikipedia links\n",
    "    if present.\n",
    "    Parameters\n",
    "    ----------\n",
    "    urlList (list) : a list of wikipedia urls\n",
    "    keep_words (integer): the number of words to keep (approx up to paragraph)\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    text_comb (string): the text extracted from the paragraphs until word limit \n",
    "                          reached\n",
    "    '''\n",
    "    text_comb = ''\n",
    "    total_words = 0\n",
    "    key_url_terms  = []\n",
    "    if len(url_wiki)> 1:\n",
    "        url_terms = [url.split('/wiki/')[1] for url in url_wiki[1:]] \n",
    "        # limiting to 3 max by the fact the url should be limited like that anyway\n",
    "        for s in url_terms:\n",
    "            s = s.replace(\"_\",\" \")\n",
    "            key_term = s.translate(str.maketrans('', '', string.punctuation))\n",
    "            key_term = \" \".join(key_term.split())\n",
    "            key_url_terms.append(key_term)\n",
    "            # print('wiki url', key_url_terms)\n",
    "        # print('url_wiki',url_wiki)\n",
    "    for url in url_wiki:\n",
    "        wiki_term = url.split('/wiki/')[1]\n",
    "        print(f\"Looking at wiki page for: {wiki_term}\")\n",
    "        try:\n",
    "            text_wiki = (wikipedia.page(wiki_term, auto_suggest = False)).content\n",
    "        except KeyError: #fullurl errors can be caused by unicode or other symbols\n",
    "            text_wiki = (wikipedia.page(wiki_term, auto_suggest = True)).content\n",
    "        #This will drop headers surrounded by ==\n",
    "        text_wiki = re.sub(r'==.*?==+', '', text_wiki)\n",
    "        paras = text_wiki.split('\\n\\n')\n",
    "        word_count = len(paras[0].split()) #number of words in 1st paragraph\n",
    "        remaining_words = keep_words - total_words\n",
    "        j = 0\n",
    "        text = paras[0]\n",
    "        while word_count < remaining_words and j<len(paras)-1:\n",
    "            j += 1\n",
    "            para_text = paras[j]\n",
    "            word_count = word_count + len(para_text.split())\n",
    "            text = text + ' ' + para_text\n",
    "        #Drop new line /n clutter\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = re.sub(\"\\s\\s+\", \" \", text)\n",
    "        text_comb = text_comb + text # change if want more than one\n",
    "        total_words = total_words + word_count\n",
    "        if total_words >= keep_words:\n",
    "            break  # break out of for loop when we have enough words\n",
    "\n",
    "    return text_comb, key_url_terms\n",
    "  \n",
    "def wiki_autosuggest(input, keep_words = 10000, suggest = False):\n",
    "    ''' \n",
    "    Gets text from Wikipedia using whichever page is found and cleans up the text\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    input (string): original input word\n",
    "    keep_words (integer): number of words to keep\n",
    "    suggest (Boolean): suggest = True means use wikipedia autosuggest function,\n",
    "                       False takes the input as is to find the page\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    text (string): text that has been cleaned up\n",
    "               \n",
    "    '''\n",
    "    # Get text from single wikipedia page\n",
    "    try:\n",
    "        text_wiki = (wikipedia.page(input, auto_suggest = suggest)).content\n",
    "    except Exception as err:\n",
    "        print(err.args)\n",
    "        raise ValueError(f'No urls found for {input}') \n",
    "    #if no exception raised clean up text\n",
    "    #This will drop headers surrounded by ==\n",
    "    text_wiki = re.sub(r'==.*?==+', '', text_wiki)\n",
    "    paras = text_wiki.split('\\n\\n')\n",
    "    word_count=len(paras[0].split()) #number of words in 1st paragraph\n",
    "    j=0\n",
    "    text = paras[0]\n",
    "    while word_count < keep_words and j<len(paras)-1:\n",
    "        j += 1\n",
    "        para_text = paras[j]\n",
    "        word_count = word_count + len(para_text.split())\n",
    "        text = text + ' ' + para_text\n",
    "    #Drop new line /n clutter\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub(\"\\s\\s+\", \" \", text)\n",
    "    return text\n",
    "  \n",
    "def find_text(input, keep_words=10000, cleanup=True, multi_links = True): \n",
    "    '''  \n",
    "    Finds text related to input that can be used for keyword extraction. \n",
    "    The text is found in the following order, using wikipedia directly without \n",
    "    autosuggest (as autosuggest can sometimes have a weird error, e.g Belfast),\n",
    "    wikipedia using autosuggest, then tries using knowledge graph to find wiki links\n",
    "    This function attempts to clean up the relevant text if cleanup is set to True.\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "  \n",
    "    input (string): input word (final answer in Linkee)\n",
    "    keep_words (integer): the number of words (+to end of paragraph) to keep in text.\n",
    "    multi_links (Boolean): flag for using more than one wiki page\n",
    "    text (string): the block of text extracted from wiki\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "  \n",
    "    key_url_terms (list): the page names of any pages used to add to keywords\n",
    "    '''\n",
    "    key_url_terms = []\n",
    "    try:\n",
    "        text = wiki_autosuggest(input, keep_words = keep_words, suggest = False)\n",
    "    except Exception as ex:\n",
    "        print(f\"failed with {ex}\")\n",
    "        try:\n",
    "            text = wiki_autosuggest(input, keep_words = keep_words, suggest = True)\n",
    "        except Exception as e:\n",
    "            print(f\"failed with {e}\")\n",
    "            knowledge_graph_df = get_knowledge_graph_df(input)\n",
    "            if len(knowledge_graph_df) == 0:\n",
    "                print(\"No valid keyword\")\n",
    "                # print(\"nothing found using knowledge graph, trying wiki\")\n",
    "                return \"No valid keyword\", []\n",
    "                #keyword vs \n",
    "            else:\n",
    "                # Try to get wiki pages from knowledge graph\n",
    "                urlList = collect_urls(knowledge_graph_df)\n",
    "                url_wiki = get_wiki_links(urlList)\n",
    "                if len(url_wiki) >= 1:\n",
    "                    keep = min(len(url_wiki), 3)\n",
    "                    url_wiki = url_wiki[0:keep]\n",
    "                    if multi_links == False:\n",
    "                        url_wiki = url_wiki[0:1]\n",
    "                        text, key_url_terms = get_wiki_text(url_wiki, keep_words)\n",
    "                else: \n",
    "                    #Use the knowledge graph categories to find wikipedia url\n",
    "                    print(\" No wiki urls: 1st pass\")\n",
    "                    category = classify_input(knowledge_graph_df)\n",
    "                    search_input = tailored_search(category, input)\n",
    "                    print(f\"Searching for urls with input {search_input}\")\n",
    "                    urlList = collect_urls(get_knowledge_graph_df(search_input))\n",
    "                    url_wiki = get_wiki_links(urlList)\n",
    "                    if len(url_wiki) >= 1:\n",
    "                        keep = min(len(url_wiki), 3)\n",
    "                        url_wiki = url_wiki[0:keep]\n",
    "                        if multi_links == False:\n",
    "                            url_wiki = url_wiki[0:1]\n",
    "                        text, key_url_terms  = get_wiki_text(url_wiki, keep_words)\n",
    "                    else:\n",
    "                        print(\"no wiki pages found\")\n",
    "                        return \"No valid keyword\", []\n",
    "                        # print(\" No wiki urls: 2nd pass\")\n",
    "                        # text = wiki_autosuggest(input)\n",
    "    \n",
    "  \n",
    "    #Text Cleaning\n",
    "    text = re.sub(r\"\\'\", '', text) #Get rid of \\'\n",
    "    text = re.sub(r\"\\\\xa0...\", '', text) #Get rid of \\\\xa0...\n",
    "    text = re.sub(r\"\\\\n\", ' ', text) #Get rid of \\\\n\n",
    "    text = re.sub(r\"\\\\u200e\", ' ', text) #Get rid of \\\\u200e\n",
    "    text = re.sub(r\"U S \", \"US \", text)\n",
    "    text = re.sub(r\"logo\", '', text)\n",
    "    text = re.sub(r\"[Vv]iew \\d+ more rows\", '', text) #Get rid of [Vv]iew \\d+ more rows\n",
    "    text = re.sub(r\"\\d+ hours ago\", '', text)\n",
    "    #Remove things like \"2009.Power\" - no space after full stop\n",
    "    rx = r\"\\.(?=[A-Za-z])\"\n",
    "    text = re.sub(rx, \". \", text)\n",
    "    if cleanup == True:\n",
    "        text = re.sub(r\"[\\\"\\'\\“\\”\\[\\]\\)\\(\\•\\▽\\❖\\†]+\", '', text)\n",
    "        text = re.sub(r\"[-·—,.;:@#?!$+-]+\", ' ', text) \n",
    "    \n",
    "    text = ' '.join(text.split()) #Single spacing\n",
    "  \n",
    "    return text, key_url_terms\n",
    "\n",
    "def keyword_extract(text, ngram_size):\n",
    "    '''Extract keywords/phrases of ngram_size using YAKE'''\n",
    "    #Initialise extractor\n",
    "    kw_extractor = yake.KeywordExtractor()\n",
    "    language = \"en\"\n",
    "    max_ngram_size = ngram_size\n",
    "    deduplication_threshold = 0.3\n",
    "    numOfKeywords = 100\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, \n",
    "                                                n=max_ngram_size, \n",
    "                                                dedupLim=deduplication_threshold, \n",
    "                                                top=numOfKeywords, features=None)\n",
    "    \n",
    "    #Run extractor on text and get out words/phrases\n",
    "    yake_output = custom_kw_extractor.extract_keywords(text)\n",
    "    words, scores = zip(*yake_output)\n",
    "    words = list(words)\n",
    "    scores = list(scores)\n",
    "    words = [re.sub(r\"[,.;@#?!$]+\", ' ', i) for i in words]\n",
    "    return(words,scores)\n",
    "  \n",
    "def answer_keyword_compare(keywords_list, input_words):\n",
    "    '''Remove candidate keywords that contain input words'''\n",
    "  \n",
    "    keywords_list = [x for x in keywords_list if not any(i in input_words for i in x.split())]\n",
    "    return keywords_list\n",
    "  \n",
    "def remove_non_noun_full_keywords(keywords_list):\n",
    "    '''\n",
    "    Only retain keywords/keyphrases that are proper nouns.\n",
    "    '''\n",
    "    pos = nltk.pos_tag(keywords_list)\n",
    "    new_keyword_list = []\n",
    "    for ii in np.arange(0,len(pos),1):\n",
    "        if pos[ii][1]=='NNP':\n",
    "            new_keyword_list.append(pos[ii][0])\n",
    "        if pos[ii][1]=='NNPS':\n",
    "            new_keyword_list.append(pos[ii][0])\n",
    "    return new_keyword_list\n",
    "  \n",
    "def select_keywords(words2):\n",
    "    '''\n",
    "    Selects the keywords/phrases to use for question generation. Ensures that \n",
    "    keyword phrases do not overlap each other.\n",
    "    '''\n",
    "    words3 = []\n",
    "    words3.append(words2[0])\n",
    "    del words2[0]\n",
    "    for i in range(len(words2)):\n",
    "        #If at any point, we only have 4 candidate keywords left, use them all\n",
    "        if len(words2) + len(words3) <= 4:\n",
    "            words3 = words3 + words2\n",
    "            break\n",
    "        test_words = words2[0].lower().split()\n",
    "        singles = [singularize(plural) for plural in test_words]\n",
    "        plurals1 = [pluralize(single) for single in singles]\n",
    "        plurals2 = [ending_pluralize(single) for single in singles]\n",
    "        plurals3 = [add_s_pluralize(single) for single in singles]\n",
    "        test_words = list(set(test_words + singles + plurals1 + plurals2 + plurals3))\n",
    "        previous_words = words3.copy()\n",
    "        previous_words = [word for phrase in previous_words for word in phrase.split()]\n",
    "        previous_words = [x.lower() for x in previous_words]\n",
    "        if len(test_words) + len(previous_words) == len(list(set(test_words + previous_words))):\n",
    "            words3.append(words2[0])\n",
    "        del words2[0]\n",
    "    return(words3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "762ec0d2-bbf4-4839-801c-1b0802e97958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>@type</th>\n",
       "      <th>resultScore</th>\n",
       "      <th>result.image.url</th>\n",
       "      <th>result.image.contentUrl</th>\n",
       "      <th>result.name</th>\n",
       "      <th>result.url</th>\n",
       "      <th>result.@type</th>\n",
       "      <th>result.description</th>\n",
       "      <th>result.detailedDescription.url</th>\n",
       "      <th>result.detailedDescription.license</th>\n",
       "      <th>result.detailedDescription.articleBody</th>\n",
       "      <th>result.@id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>4592.872070</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:France...</td>\n",
       "      <td>https://encrypted-tbn1.gstatic.com/images?q=tb...</td>\n",
       "      <td>France national football team</td>\n",
       "      <td>http://www.fff.fr/bleus/</td>\n",
       "      <td>[Thing, SportsTeam]</td>\n",
       "      <td>Football team</td>\n",
       "      <td>https://en.wikipedia.org/wiki/France_national_...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>The France national football team represents F...</td>\n",
       "      <td>kg:/m/01l3vx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>2954.216553</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:Ligue1...</td>\n",
       "      <td>https://encrypted-tbn3.gstatic.com/images?q=tb...</td>\n",
       "      <td>Ligue 1</td>\n",
       "      <td>http://www.ligue1.com/</td>\n",
       "      <td>[Corporation, Thing, Organization, SportsOrgan...</td>\n",
       "      <td>Football league</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Ligue_1</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>Ligue 1, officially known as Ligue 1 Uber Eats...</td>\n",
       "      <td>kg:/m/044hxl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>2823.790039</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:Leroy_...</td>\n",
       "      <td>https://encrypted-tbn3.gstatic.com/images?q=tb...</td>\n",
       "      <td>Leroy Merlin</td>\n",
       "      <td>http://www.leroymerlin.com/</td>\n",
       "      <td>[Organization, Thing, Corporation]</td>\n",
       "      <td>Retail company</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Leroy_Merlin</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>Leroy Merlin is a French headquartered home im...</td>\n",
       "      <td>kg:/m/04y8tkw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>2259.376709</td>\n",
       "      <td>https://fr.m.wikipedia.org/wiki/Fichier:Orange...</td>\n",
       "      <td>https://encrypted-tbn1.gstatic.com/images?q=tb...</td>\n",
       "      <td>Orange S.A.</td>\n",
       "      <td>http://www.orange.com</td>\n",
       "      <td>[Organization, Thing, Corporation]</td>\n",
       "      <td>Telecom company</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Orange_S.A.</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>Orange S.A., rebranded as Orange, formerly Fra...</td>\n",
       "      <td>kg:/m/0jd05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>2098.627930</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:Air_Fr...</td>\n",
       "      <td>https://encrypted-tbn3.gstatic.com/images?q=tb...</td>\n",
       "      <td>Air France</td>\n",
       "      <td>http://www.airfrance.com</td>\n",
       "      <td>[Airline, Thing, Organization, Corporation]</td>\n",
       "      <td>Air carrier</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Air_France</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>Air France, stylised as AIRFRANCE, is the flag...</td>\n",
       "      <td>kg:/m/0h7k5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>1986.733521</td>\n",
       "      <td>https://fr.m.wikipedia.org/wiki/Fichier:Caa-co...</td>\n",
       "      <td>https://encrypted-tbn2.gstatic.com/images?q=tb...</td>\n",
       "      <td>Crédit Agricole</td>\n",
       "      <td>http://www.credit-agricole.com/</td>\n",
       "      <td>[Organization, Thing, Corporation]</td>\n",
       "      <td>Bank</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cr%C3%A9dit_Agri...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>Crédit Agricole Group, sometimes called La ban...</td>\n",
       "      <td>kg:/m/028vbm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>1880.628906</td>\n",
       "      <td>https://cs.m.wikipedia.org/wiki/Soubor:Meteo_F...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>Météo-France</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Organization, Thing, GovernmentOrganization]</td>\n",
       "      <td>Meteorological service</td>\n",
       "      <td>https://en.wikipedia.org/wiki/M%C3%A9t%C3%A9o-...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>Météo-France is the French national meteorolog...</td>\n",
       "      <td>kg:/m/0ckgrh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>1779.085693</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:Ouest-...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>Ouest-France</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Periodical, Thing]</td>\n",
       "      <td>Newspaper</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Ouest-France</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>Ouest-France is a daily French newspaper known...</td>\n",
       "      <td>kg:/m/03kqgp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>1716.644653</td>\n",
       "      <td>https://en.wikipedia.org/wiki/France_Info_(rad...</td>\n",
       "      <td>https://encrypted-tbn3.gstatic.com/images?q=tb...</td>\n",
       "      <td>France Info</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Organization, Thing]</td>\n",
       "      <td>Radio network</td>\n",
       "      <td>https://en.wikipedia.org/wiki/France_Info_(rad...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>France Info is a radio network operated by the...</td>\n",
       "      <td>kg:/m/07nhv9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>1700.724854</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:Logo_M...</td>\n",
       "      <td>https://encrypted-tbn2.gstatic.com/images?q=tb...</td>\n",
       "      <td>Maisons du Monde</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Organization, Thing, Corporation]</td>\n",
       "      <td>Company</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Maisons_du_Monde</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>Maisons du Monde is a French furniture and hom...</td>\n",
       "      <td>kg:/g/122044ns</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                @type  resultScore  \\\n",
       "0  EntitySearchResult  4592.872070   \n",
       "1  EntitySearchResult  2954.216553   \n",
       "2  EntitySearchResult  2823.790039   \n",
       "3  EntitySearchResult  2259.376709   \n",
       "4  EntitySearchResult  2098.627930   \n",
       "5  EntitySearchResult  1986.733521   \n",
       "6  EntitySearchResult  1880.628906   \n",
       "7  EntitySearchResult  1779.085693   \n",
       "8  EntitySearchResult  1716.644653   \n",
       "9  EntitySearchResult  1700.724854   \n",
       "\n",
       "                                    result.image.url  \\\n",
       "0  https://commons.wikimedia.org/wiki/File:France...   \n",
       "1  https://commons.wikimedia.org/wiki/File:Ligue1...   \n",
       "2  https://commons.wikimedia.org/wiki/File:Leroy_...   \n",
       "3  https://fr.m.wikipedia.org/wiki/Fichier:Orange...   \n",
       "4  https://commons.wikimedia.org/wiki/File:Air_Fr...   \n",
       "5  https://fr.m.wikipedia.org/wiki/Fichier:Caa-co...   \n",
       "6  https://cs.m.wikipedia.org/wiki/Soubor:Meteo_F...   \n",
       "7  https://commons.wikimedia.org/wiki/File:Ouest-...   \n",
       "8  https://en.wikipedia.org/wiki/France_Info_(rad...   \n",
       "9  https://commons.wikimedia.org/wiki/File:Logo_M...   \n",
       "\n",
       "                             result.image.contentUrl  \\\n",
       "0  https://encrypted-tbn1.gstatic.com/images?q=tb...   \n",
       "1  https://encrypted-tbn3.gstatic.com/images?q=tb...   \n",
       "2  https://encrypted-tbn3.gstatic.com/images?q=tb...   \n",
       "3  https://encrypted-tbn1.gstatic.com/images?q=tb...   \n",
       "4  https://encrypted-tbn3.gstatic.com/images?q=tb...   \n",
       "5  https://encrypted-tbn2.gstatic.com/images?q=tb...   \n",
       "6  https://encrypted-tbn0.gstatic.com/images?q=tb...   \n",
       "7  https://encrypted-tbn0.gstatic.com/images?q=tb...   \n",
       "8  https://encrypted-tbn3.gstatic.com/images?q=tb...   \n",
       "9  https://encrypted-tbn2.gstatic.com/images?q=tb...   \n",
       "\n",
       "                     result.name                       result.url  \\\n",
       "0  France national football team         http://www.fff.fr/bleus/   \n",
       "1                        Ligue 1           http://www.ligue1.com/   \n",
       "2                   Leroy Merlin      http://www.leroymerlin.com/   \n",
       "3                    Orange S.A.            http://www.orange.com   \n",
       "4                     Air France         http://www.airfrance.com   \n",
       "5                Crédit Agricole  http://www.credit-agricole.com/   \n",
       "6                   Météo-France                              NaN   \n",
       "7                   Ouest-France                              NaN   \n",
       "8                    France Info                              NaN   \n",
       "9               Maisons du Monde                              NaN   \n",
       "\n",
       "                                        result.@type      result.description  \\\n",
       "0                                [Thing, SportsTeam]           Football team   \n",
       "1  [Corporation, Thing, Organization, SportsOrgan...         Football league   \n",
       "2                 [Organization, Thing, Corporation]          Retail company   \n",
       "3                 [Organization, Thing, Corporation]         Telecom company   \n",
       "4        [Airline, Thing, Organization, Corporation]             Air carrier   \n",
       "5                 [Organization, Thing, Corporation]                    Bank   \n",
       "6      [Organization, Thing, GovernmentOrganization]  Meteorological service   \n",
       "7                                [Periodical, Thing]               Newspaper   \n",
       "8                              [Organization, Thing]           Radio network   \n",
       "9                 [Organization, Thing, Corporation]                 Company   \n",
       "\n",
       "                      result.detailedDescription.url  \\\n",
       "0  https://en.wikipedia.org/wiki/France_national_...   \n",
       "1              https://en.wikipedia.org/wiki/Ligue_1   \n",
       "2         https://en.wikipedia.org/wiki/Leroy_Merlin   \n",
       "3          https://en.wikipedia.org/wiki/Orange_S.A.   \n",
       "4           https://en.wikipedia.org/wiki/Air_France   \n",
       "5  https://en.wikipedia.org/wiki/Cr%C3%A9dit_Agri...   \n",
       "6  https://en.wikipedia.org/wiki/M%C3%A9t%C3%A9o-...   \n",
       "7         https://en.wikipedia.org/wiki/Ouest-France   \n",
       "8  https://en.wikipedia.org/wiki/France_Info_(rad...   \n",
       "9     https://en.wikipedia.org/wiki/Maisons_du_Monde   \n",
       "\n",
       "                  result.detailedDescription.license  \\\n",
       "0  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "1  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "2  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "3  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "4  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "5  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "6  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "7  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "8  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "9  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "\n",
       "              result.detailedDescription.articleBody      result.@id  \n",
       "0  The France national football team represents F...    kg:/m/01l3vx  \n",
       "1  Ligue 1, officially known as Ligue 1 Uber Eats...    kg:/m/044hxl  \n",
       "2  Leroy Merlin is a French headquartered home im...   kg:/m/04y8tkw  \n",
       "3  Orange S.A., rebranded as Orange, formerly Fra...     kg:/m/0jd05  \n",
       "4  Air France, stylised as AIRFRANCE, is the flag...     kg:/m/0h7k5  \n",
       "5  Crédit Agricole Group, sometimes called La ban...    kg:/m/028vbm  \n",
       "6  Météo-France is the French national meteorolog...    kg:/m/0ckgrh  \n",
       "7  Ouest-France is a daily French newspaper known...    kg:/m/03kqgp  \n",
       "8  France Info is a radio network operated by the...    kg:/m/07nhv9  \n",
       "9  Maisons du Monde is a French furniture and hom...  kg:/g/122044ns  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_knowledge_graph_df(\"France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90e1b960-5ba9-4b2c-b8e7-f6813350b968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkee_keywords(input):\n",
    "    \"\"\"\n",
    "    Main pipeline function which takes input and generates list of keywords using \n",
    "    wikipedia scraping and NLP.\n",
    "  \n",
    "    Parameters\n",
    "    ---------\n",
    "    input (string): the input word which is the final Linkee answer\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    final_keywords (list): the list of possible keywords\n",
    "    \"\"\"\n",
    "    answer_list = tidy_input(input)\n",
    "  \n",
    "    #Keyword extraction\n",
    "    text, key_url_terms = find_text(input)\n",
    "    \n",
    "    #Potentially here combine 1-gram, 2-gram, 3-gram results\n",
    "    words_df = pd.DataFrame()\n",
    "    words = (keyword_extract(text, 2))[0] #+ (keyword_extract(text, 1))[0] + (keyword_extract(text, 3))[0]\n",
    "    scores = (keyword_extract(text, 2))[1] #+ (keyword_extract(text, 1))[1] + (keyword_extract(text, 3))[1]\n",
    "    words_df['words'] = words\n",
    "    words_df['scores'] = scores\n",
    "    words_df.sort_values(by=['scores'],ascending=False)\n",
    "    words_df = words_df[:100]\n",
    "  \n",
    "    # Adding wikipedia page names to the words found at the top\n",
    "    words = key_url_terms + words\n",
    "  \n",
    "    #Cleaning up returned keywords\n",
    "    words2 = answer_keyword_compare(words, answer_list)\n",
    "  \n",
    "    words2 = remove_non_noun_full_keywords(words2)\n",
    "  \n",
    "    final_keywords = select_keywords(words2)\n",
    "  \n",
    "    return(final_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8989c6f2-39dd-487f-a8e8-8ca7a891fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleanup function to clean up the fact at the end\n",
    "def cleanup(s):\n",
    "    \"\"\" Cleans up string by removing certain characters \"\"\"\n",
    "    strip_refs = re.compile(\"\\.?\\[\\d+\\]?\")\n",
    "    s = strip_refs.sub(\"\", s).strip()\n",
    "    \n",
    "    # Pretty ugly\n",
    "    if s[-1] == \".\":\n",
    "        s = s[0:-1]\n",
    "        \n",
    "    return s\n",
    "\n",
    "def fill_in_blank_q_generate(final_input, input, facts = 1):\n",
    "    #Might break if only 1 fact possible and not 2 - needs fixed\n",
    "    \"\"\"\n",
    "    Generate a fill in the blank style question using 1 or 2 \n",
    "    facts about the input.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    final_input (string): original input (final answer) for card\n",
    "    input (string): answer/keyword which we want to generate question for\n",
    "    facts (integer 1/2): allows to keep 1 or 2 facts for each answer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    question (\"string\"): the fill in the blank question\n",
    "    num_statements (integer): the number of statements found corresponding to the \n",
    "                            input\n",
    "    \"\"\"\n",
    "    final_input = tidy_input(final_input)\n",
    "\n",
    "    if (facts < 1) or (facts > 2):\n",
    "        return(\"Invalid entry. Please specify if you want 1 or 2 facts.\")\n",
    "    page_name = input\n",
    "\n",
    "    #Get Wikipedia page and text\n",
    "    #using knowledge graph method\n",
    "    text, key_url = find_text(page_name, keep_words= 100000, cleanup = False, multi_links = False)\n",
    "    print(page_name)\n",
    "    # text = (wikipedia.page(page_name,auto_suggest=False)).content \n",
    "    text = re.sub(r'==.*?==+', '', text)\n",
    "    #text = text.replace('\\n', '')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub(\"\\s\\s+\", \" \", text)\n",
    "\n",
    "    #Get category of input to better search for entity\n",
    "    category = classify_input(get_knowledge_graph_df(input))\n",
    "    #If person, just look for surname when trying to find facts\n",
    "    if category == \"Person\":\n",
    "        page_entity = input.split()[-1]\n",
    "    else:\n",
    "        page_entity = input\n",
    "    #Above is fine\n",
    "\n",
    "    doc = nlp(text)\n",
    "    #Set up empty array to hold facts\n",
    "    uniqueStatements = []\n",
    "    #cue: Verb lemma with which ``entity`` is associated (e.g. \"be\", \"have\", \"say\").\n",
    "    #Potentially use pronouns for the entity as well and combine results\n",
    "    for cue in [\"be\", \"have\", \"say\", \"do\", \"win\", \"write\", \"talk\", \"talk about\", \"born\", \"receive\", \"make\", \"continue\", \"find\"]:\n",
    "        statements = textacy.extract.semistructured_statements(doclike = doc, entity = page_entity, cue = cue)\n",
    "        for statement in statements:\n",
    "            entity, verb, fact = statement\n",
    "            factlist = [str(word) for word in fact]\n",
    "            fact = cleanup(str(fact))\n",
    "\n",
    "            #Removing statements where target word/phrase appears in fact\n",
    "            # if re.search(page_name, fact):\n",
    "            #   continue\n",
    "      \n",
    "            # if len(answer_keyword_compare(factlist, page_name)) != \\\n",
    "            #    len(factlist):\n",
    "            #    print(f'removed: {factlist} due to page_name {page_name} ')\n",
    "            #if fact has answer word in it ignore\n",
    "            #  continue\n",
    "            if len(answer_keyword_compare(factlist, final_input)) != len(factlist):\n",
    "                #if fact has final answer in it ignore \n",
    "                print(f'removed: {factlist} due to final_input {final_input} ')\n",
    "                continue\n",
    "            #Remove statements that are too long - more than 35 words long\n",
    "            if len(fact.split()) > 35:\n",
    "                continue\n",
    "            elif len(fact.split()) < 5:\n",
    "                continue\n",
    "            # statement = f\"{page_name} {verb} {fact}\"\n",
    "            statement = f\"{verb} {fact}\"\n",
    "            #More cleanup on fact\n",
    "            statement = re.sub(r\"[\\[\\]\\•\\▽\\❖\\†]+\", '', statement)\n",
    "            statement = statement.replace(', ', ' ')\n",
    "            statement = statement.replace(' , ', ', ')\n",
    "            statement = statement.replace(' ( ', ' (')\n",
    "            statement = statement.replace(' )', ')')\n",
    "            statement = statement.replace(\" 's\", \"'s\")\n",
    "            statement = statement.replace(\" - \", \"-\")\n",
    "            statement = statement.replace(\"\\'s\", \"'s\")\n",
    "            statement = statement.replace(page_name, '______')\n",
    "            statement = f\"{page_name} {statement}\"\n",
    "\n",
    "            uniqueStatements.append(statement)\n",
    "    num_statements = len(uniqueStatements)\n",
    "\n",
    "    #If it can't find any facts, should we try splitting up the input in\n",
    "    #a different manner - at moment, just telling us this is happening\n",
    "    if len(uniqueStatements) == 0:\n",
    "        return(\"No facts found for input.\", num_statements)\n",
    "\n",
    "    #Ensure code doesn't break if 2 facts are asked for but not available\n",
    "    if len(uniqueStatements) == 1 and facts == 2:\n",
    "        print('Only one fact available for answer.')\n",
    "        facts = 1\n",
    "\n",
    "    #Good tags for finding facts are numbers, proper nouns,\n",
    "    #foreign words and comparative/superlative adjectives/adverbs\n",
    "    good_tags = ['CD', 'FW', 'JJR', 'JJS', 'NNP', 'NNPS', 'RBR', 'RBRS']\n",
    "    tag_count = []\n",
    "    for i in range(len(uniqueStatements)):\n",
    "        tag_tuples = nltk.pos_tag(uniqueStatements[i].split())\n",
    "        tags = [x[1] for x in tag_tuples]\n",
    "        #Adding a small weight for statement length to prioritise longer facts \n",
    "        #which should have more info\n",
    "        tag_count.append(sum(x in good_tags for x in tags) + 0.3*len(tags))\n",
    "\n",
    "    #Returning a sorted DataFrame of all the questions to be able to view\n",
    "    df = pd.DataFrame(list(zip(uniqueStatements, tag_count)),\n",
    "                      columns =['Statement', 'Count'])\n",
    "    df = df.sort_values(by = 'Count', ascending = False)\n",
    "    return(df)\n",
    "\n",
    "    #Get sorted array of indexes containing facts in ascending order of good tags\n",
    "    #In case of a tie, this arrays puts the lower numbered index first\n",
    "    sorted_count = sorted(range(len(tag_count)), key=lambda k: tag_count[k])\n",
    "\n",
    "    #Use 2 facts with most good tags in them\n",
    "\n",
    "    fact1 = uniqueStatements[sorted_count[-1]]\n",
    "    if facts == 2:\n",
    "        fact2 = uniqueStatements[sorted_count[-2]]\n",
    "\n",
    "    #Calculate how many letters are in the answer we are blanking\n",
    "    page_name_words = page_name\n",
    "    words = page_name_words.split()\n",
    "    letters_per_word = [len(w) for w in words]\n",
    "\n",
    "    fact1 = fact1.replace(page_name, '______ ' + str(letters_per_word))\n",
    "    if category == \"Person\" and facts == 2:\n",
    "        fact2 = fact2.replace(page_name, 'This person')\n",
    "    elif category != \"Person\" and facts == 2:\n",
    "        fact2 = fact2.replace(page_name, 'It')\n",
    "\n",
    "    if facts == 1:\n",
    "        question = str('Fill in the blank: ') + fact1 + str('.')\n",
    "    elif facts == 2:\n",
    "        question = str('Fill in the blank: ') + fact1 + str('. ') + fact2 + str('.')\n",
    "\n",
    "    return(question, num_statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7711466c-85a3-4cb0-abad-f372165e9f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_card(input):\n",
    "    '''Takes input and generates 4 question answer pairs'''\n",
    "    keywords = linkee_keywords(input)\n",
    "    answers = [] #Empty list to add answers we have questions for\n",
    "    questions = []\n",
    "    for answer in keywords:\n",
    "        try:\n",
    "            question, num_statements = fill_in_blank_q_generate(input, answer, facts = 2)\n",
    "        #except PageError: \n",
    "        except: \n",
    "            print(answer + ' does not have a Wikipedia page')\n",
    "            continue\n",
    "\n",
    "        if question == 'No facts found for input.':\n",
    "            print(answer + ' does not have facts')\n",
    "            continue\n",
    "        else:\n",
    "            print(answer + ' does have facts')\n",
    "            answers.append(answer)\n",
    "            questions.append(question)\n",
    "  \n",
    "    for i in range(len(answers)):\n",
    "        questions = [re.sub(answers[i], f\"[keyword {i+1}]\", qus) for qus in questions]\n",
    "    print(answers,\"; \",questions)\n",
    "    return(answers, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45aee657-eab2-4b4b-b322-48d568bb446e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past here Academy Award\n",
      "Academy Award does not have facts\n",
      "past here Golden Globe\n",
      "Golden Globe does not have facts\n",
      "('Saving Private',)\n",
      "failed with No urls found for Saving Private\n",
      "past here Saving Private\n",
      "Saving Private does not have facts\n",
      "past here American actor\n",
      "American actor does not have facts\n",
      "past here Forrest Gump\n",
      "removed: ['six', 'Academy', 'Awards', ':', 'Best', 'Picture', ',', 'Best', 'Director', ',', 'Best', 'Actor', 'for', 'Hanks', ',', 'Best', 'Adapted', 'Screenplay', ',', 'Best', 'Visual', 'Effects', ',', 'and', 'Best', 'Film', 'Editing'] due to final_input ['toms', 'HANK', 'Toms', 'tom', 'HANKS', 'Hank', 'TOMS', 'hank', 'Tom', 'hanks', 'Hanks', 'TOM'] \n",
      "Forrest Gump does have facts\n",
      "('Lucky Guy', ['Lucky Guy (play)', 'Lucky Guy (musical)', '\"Lucky Guy\" (Dieter Bohlen song)', '\"Lucky Guy\" (Kim Hyun-joong song)', 'The Lucky Guy'])\n",
      "failed with No urls found for Lucky Guy\n",
      "past here Lucky Guy\n",
      "Only one fact available for answer.\n",
      "Lucky Guy does have facts\n",
      "past here North America\n",
      "North America does have facts\n",
      "past here Beautiful Day\n",
      "Beautiful Day does have facts\n",
      "past here Robert Langdon\n",
      "Robert Langdon does have facts\n",
      "past here Sheriff Woody\n",
      "Only one fact available for answer.\n",
      "Sheriff Woody does have facts\n",
      "past here Walt Disney\n",
      "Walt Disney does not have facts\n",
      "('Night Live',)\n",
      "failed with No urls found for Night Live\n",
      "past here Night Live\n",
      "Night Live does have facts\n",
      "('Life Achievement',)\n",
      "failed with No urls found for Life Achievement\n",
      "past here Life Achievement\n",
      "Life Achievement does not have facts\n",
      "('Cloud atlas (disambiguation)', ['International Cloud Atlas', 'Alexander George McAdie', 'Donald Platt', 'Cloud Atlas (novel)', 'Liam Callanan', 'Cloud Atlas (film)', 'Toshi Ichiyanagi', 'Mikel Rouse'])\n",
      "failed with No urls found for Cloud Atlas\n",
      "('could atlas',)\n",
      "failed with No urls found for Cloud Atlas\n",
      "Looking at wiki page for: Cloud_Atlas_(film)\n",
      "past here Cloud Atlas\n",
      "removed: ['a', '2012', 'epic', 'science', 'fiction', 'film', 'written', 'and', 'directed', 'by', 'the', 'Wachowskis', 'and', 'Tom', 'Tykwer'] due to final_input ['toms', 'HANK', 'Toms', 'tom', 'HANKS', 'Hank', 'TOMS', 'hank', 'Tom', 'hanks', 'Hanks', 'TOM'] \n",
      "Only one fact available for answer.\n",
      "Cloud Atlas does have facts\n",
      "('Brothers John',)\n",
      "failed with No urls found for Brothers John\n",
      "past here Brothers John\n",
      "Brothers John does not have facts\n",
      "past here Sony Pictures\n",
      "Sony Pictures does not have facts\n",
      "past here Fred Rogers\n",
      "Fred Rogers does not have facts\n",
      "past here Kennedy Center\n",
      "Kennedy Center does have facts\n",
      "past here Oakland California\n",
      "Oakland California does not have facts\n",
      "('Stone magazine',)\n",
      "failed with No urls found for Stone magazine\n",
      "past here Stone magazine\n",
      "Stone magazine does not have facts\n",
      "('Public Outreach',)\n",
      "failed with No urls found for Public Outreach\n",
      "past here Public Outreach\n",
      "Public Outreach does not have facts\n",
      "('Kip Wilson',)\n",
      "failed with No urls found for Kip Wilson\n",
      "past here Kip Wilson\n",
      "Kip Wilson does not have facts\n",
      "past here Greek Orthodox\n",
      "Greek Orthodox does not have facts\n",
      "('Philadelphias success',)\n",
      "failed with No urls found for Philadelphias success\n",
      "past here Philadelphias success\n",
      "Philadelphias success does not have facts\n",
      "('Oscars Spencer',)\n",
      "failed with No urls found for Oscars Spencer\n",
      "('oscar spencer',)\n",
      "failed with No urls found for Oscars Spencer\n",
      "Looking at wiki page for: Charlie_Chaplin\n",
      "past here Oscars Spencer\n",
      "Oscars Spencer does not have facts\n",
      "past here Joe Wright\n",
      "Joe Wright does have facts\n",
      "('Museum Capital',)\n",
      "failed with No urls found for Museum Capital\n",
      "past here Museum Capital\n",
      "Museum Capital does not have facts\n",
      "past here Premier League\n",
      "Premier League does have facts\n",
      "past here Apollo program\n",
      "Only one fact available for answer.\n",
      "Apollo program does have facts\n",
      "('Finch directed',)\n",
      "failed with No urls found for Finch directed\n",
      "('final director',)\n",
      "failed with No urls found for Finch directed\n",
      "Looking at wiki page for: All_the_Bright_Places_(film)\n",
      "past here Finch directed\n",
      "Finch directed does not have facts\n",
      "past here Tennessee Williams\n",
      "Tennessee Williams does have facts\n",
      "past here Larry Crowne\n",
      "removed: ['a', '2011', 'American', 'romantic', 'comedy', 'film', 'starring', 'Tom', 'Hanks', 'and', 'Julia', 'Roberts'] due to final_input ['toms', 'HANK', 'Toms', 'tom', 'HANKS', 'Hank', 'TOMS', 'hank', 'Tom', 'hanks', 'Hanks', 'TOM'] \n",
      "Larry Crowne does not have facts\n",
      "past here Prime Minister\n",
      "Only one fact available for answer.\n",
      "Prime Minister does have facts\n",
      "('Zemeckiss Cast',)\n",
      "failed with No urls found for Zemeckiss Cast\n",
      "('zemeckis case',)\n",
      "failed with No urls found for Zemeckiss Cast\n",
      "No valid keyword\n",
      "past here Zemeckiss Cast\n",
      "Zemeckiss Cast does not have a Wikipedia page\n",
      "past here Chabot College\n",
      "Chabot College does have facts\n",
      "('Uncommon Type',)\n",
      "failed with No urls found for Uncommon Type\n",
      "past here Uncommon Type\n",
      "Uncommon Type does not have facts\n",
      "('Ratliff announced',)\n",
      "failed with No urls found for Ratliff announced\n",
      "('ratcliffe announcer',)\n",
      "failed with No urls found for Ratliff announced\n",
      "Looking at wiki page for: Welcome_Home_(2018_film)\n",
      "past here Ratliff announced\n",
      "Ratliff announced does not have facts\n",
      "past here Broadway debut\n",
      "Broadway debut does not have facts\n",
      "past here War\n",
      "War does have facts\n",
      "('Rawley Farnsworth',)\n",
      "failed with No urls found for Rawley Farnsworth\n",
      "('ramsey farnsworth',)\n",
      "failed with No urls found for Rawley Farnsworth\n",
      "Looking at wiki page for: In_%26_Out_(film)\n",
      "past here Rawley Farnsworth\n",
      "Rawley Farnsworth does not have facts\n",
      "past here Kathleen Quinlan\n",
      "Only one fact available for answer.\n",
      "Kathleen Quinlan does have facts\n",
      "past here Frank Abagnale\n",
      "Frank Abagnale does have facts\n",
      "('Texas Congressman',)\n",
      "failed with No urls found for Texas Congressman\n",
      "past here Texas Congressman\n",
      "Texas Congressman does not have facts\n",
      "('Extremely Loud',)\n",
      "failed with No urls found for Extremely Loud\n",
      "past here Extremely Loud\n",
      "Extremely Loud does have facts\n",
      "('Luhrmann Shooting',)\n",
      "failed with No urls found for Luhrmann Shooting\n",
      "past here Luhrmann Shooting\n",
      "Luhrmann Shooting does not have a Wikipedia page\n",
      "('Chester Marlon',)\n",
      "failed with No urls found for Chester Marlon\n",
      "past here Chester Marlon\n",
      "Chester Marlon does not have facts\n",
      "('Schöneck Hesse',)\n",
      "failed with No urls found for Schöneck Hesse\n",
      "past here Schöneck Hesse\n",
      "Schöneck Hesse does not have facts\n",
      "('Takis Theodorikakos',)\n",
      "failed with No urls found for Takis Theodorikakos\n",
      "('takes theodorakakos',)\n",
      "failed with No urls found for Takis Theodorikakos\n",
      " No wiki urls: 1st pass\n",
      "Searching for urls with input Takis Theodorikakos\n",
      "no wiki pages found\n",
      "past here Takis Theodorikakos\n",
      "Takis Theodorikakos does not have facts\n",
      "('Governor Jerry',)\n",
      "failed with No urls found for Governor Jerry\n",
      "past here Governor Jerry\n",
      "Governor Jerry does not have facts\n",
      "('Portuguese descent',)\n",
      "failed with No urls found for Portuguese descent\n",
      "past here Portuguese descent\n",
      "Portuguese descent does not have facts\n",
      "past here Moon\n",
      "Moon does have facts\n",
      "past here July\n",
      "July does have facts\n",
      "('Band', ['Bánd', 'Band, Iran', 'Band, Mureș', 'Band-e Majid Khan', 'Band (surname)', 'Musical ensemble', 'Band (rock and pop)', 'Concert band', 'Dansband', 'Jazz band', 'Marching band', 'School band', 'The Band', 'The Band (album)', 'Mando Diao', 'Rede Bandeirantes', 'The Band (film)', 'The Band (musical)', 'Armband', 'Smart band', 'Microsoft Band', 'Bandolier', 'Bands (neckwear)', 'Belt (clothing)', 'Strap', 'Wedding band', 'Bands (Italian Army irregulars)', 'Female order of the Band', 'Order of the Band', 'Band (algebra)', 'Band (order theory)', 'Band (radio)', 'Frequency band', 'LTE frequency bands', 'Shortwave bands', 'UMTS frequency bands', 'BAND (software)', 'Band cell', 'Bird banding', 'Electronic band structure', 'Gastric band', 'Signaling (telecommunications)', 'In-band signaling', 'Out-of-band', 'origin of birds', 'Band (First Nations Canada)', 'Band society', 'Tribe (Native American)', 'Rubber band', 'The Band (professional wrestling)', 'All pages with titles beginning with Band', 'All pages with titles containing Band', 'Band of Brothers (disambiguation)', 'Bandage', 'Banding (disambiguation)', 'Bandy (disambiguation)', 'Bend (disambiguation)', 'Drum and bugle corps (disambiguation)', 'Herd', 'Ribbon (disambiguation)', 'Stripe (disambiguation)'])\n",
      "failed with No urls found for Band\n",
      "('and', ['Conjunction (grammar)', 'Logical conjunction', 'Bitwise AND', 'short-circuit operator', 'Ampersand', 'AND gate', 'And (John Martyn album)', 'And (Koda Kumi album)', 'A N D (Tricot album)', 'Jonah Matranga', 'Alberta New Democratic Party', 'Academy of Nutrition and Dietetics', 'Associated Northcliffe Digital', 'Automotive Navigation Data', 'AND Corporation', 'Fiverr', 'Anderson Regional Airport', 'Anderston railway station', 'Allow natural death', 'Andorra', 'Andromeda (constellation)', 'Ansus language', 'All pages with titles beginning with And', 'All pages with titles containing And', '& (disambiguation)', 'Ampersand (disambiguation)'])\n",
      "failed with No urls found for Band\n",
      "Looking at wiki page for: Ring_(jewellery)\n",
      "past here Band\n",
      "Band does not have facts\n",
      "past here Spielberg\n",
      "Spielberg does have facts\n",
      "('Keatons alcoholic',)\n",
      "failed with No urls found for Keatons alcoholic\n",
      "('keating alcohol',)\n",
      "failed with No urls found for Keatons alcoholic\n",
      "No valid keyword\n",
      "past here Keatons alcoholic\n",
      "Keatons alcoholic does not have a Wikipedia page\n",
      "past here Toy\n",
      "Toy does not have facts\n",
      "past here Emmy\n",
      "Emmy does not have facts\n",
      "past here HBO\n",
      "HBO does have facts\n",
      "['Forrest Gump', 'Lucky Guy', 'North America', 'Beautiful Day', 'Robert Langdon', 'Sheriff Woody', 'Night Live', 'Cloud Atlas', 'Kennedy Center', 'Joe Wright', 'Premier League', 'Apollo program', 'Tennessee Williams', 'Prime Minister', 'Chabot College', 'War', 'Kathleen Quinlan', 'Frank Abagnale', 'Extremely Loud', 'Moon', 'July', 'Spielberg', 'HBO'] ;  ['Statement', 'Statement', 'Statement', 'Statement', 'Statement', 'Statement', 'Statement', 'Statement', 'Statement', 'Statement', 'Statement', 'Statement', 'Statement', 'Statement', 'Statement', 'Statement', 'Statement', 'Statement', 'Statement', 'Statement', 'Statement', 'Statement', 'Statement']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Forrest Gump',\n",
       "  'Lucky Guy',\n",
       "  'North America',\n",
       "  'Beautiful Day',\n",
       "  'Robert Langdon',\n",
       "  'Sheriff Woody',\n",
       "  'Night Live',\n",
       "  'Cloud Atlas',\n",
       "  'Kennedy Center',\n",
       "  'Joe Wright',\n",
       "  'Premier League',\n",
       "  'Apollo program',\n",
       "  'Tennessee Williams',\n",
       "  'Prime Minister',\n",
       "  'Chabot College',\n",
       "  'War',\n",
       "  'Kathleen Quinlan',\n",
       "  'Frank Abagnale',\n",
       "  'Extremely Loud',\n",
       "  'Moon',\n",
       "  'July',\n",
       "  'Spielberg',\n",
       "  'HBO'],\n",
       " ['Statement',\n",
       "  'Statement',\n",
       "  'Statement',\n",
       "  'Statement',\n",
       "  'Statement',\n",
       "  'Statement',\n",
       "  'Statement',\n",
       "  'Statement',\n",
       "  'Statement',\n",
       "  'Statement',\n",
       "  'Statement',\n",
       "  'Statement',\n",
       "  'Statement',\n",
       "  'Statement',\n",
       "  'Statement',\n",
       "  'Statement',\n",
       "  'Statement',\n",
       "  'Statement',\n",
       "  'Statement',\n",
       "  'Statement',\n",
       "  'Statement',\n",
       "  'Statement',\n",
       "  'Statement'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_card('Tom Hanks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c0bd98e-19c4-464c-a123-d048889aadf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['World War',\n",
       " 'Kansas City',\n",
       " 'Gram Studio',\n",
       " 'York Times',\n",
       " 'American Broadcasting',\n",
       " 'Los Angeles',\n",
       " 'Achievement Awards',\n",
       " 'Silly Symphony',\n",
       " 'Mary Poppins',\n",
       " 'Animated Short',\n",
       " 'Chicago Academy',\n",
       " 'Tomorrow EPCOT',\n",
       " 'Experimental Prototype',\n",
       " 'Company ABC',\n",
       " 'Winter Olympics',\n",
       " 'Golden Globe',\n",
       " 'Jungle Book',\n",
       " 'Happiest Millionaire',\n",
       " 'Park School',\n",
       " 'Pinocchio Fantasia',\n",
       " 'Museum records',\n",
       " 'Iwerks',\n",
       " 'Bambi Dumbo',\n",
       " 'Roy started',\n",
       " 'Neal Gabler',\n",
       " 'South America',\n",
       " 'year theyre',\n",
       " 'Thailands Order',\n",
       " 'Pato Donald',\n",
       " 'Construction work',\n",
       " 'Virginia Davis',\n",
       " 'Red Cross',\n",
       " 'Mickey',\n",
       " 'Rudolf Ising',\n",
       " 'Carolwood Pacific',\n",
       " 'Der König',\n",
       " 'Playwright Robert',\n",
       " 'Floyd Norman',\n",
       " 'Arts colloquially',\n",
       " 'Tripp Avenue',\n",
       " 'Aztec Eagle',\n",
       " 'Alfonso Cuaron',\n",
       " 'Kenneth Branagh',\n",
       " 'Lucky Rabbit',\n",
       " 'Annette Funicello',\n",
       " 'Stephan Jungk',\n",
       " 'Movie Database',\n",
       " 'White',\n",
       " 'Kimball argues',\n",
       " 'Laugh',\n",
       " 'July',\n",
       " 'Elias']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkee_keywords('Walt Disney')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bba0cf21-af77-4506-93f3-820764406d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': {'code': 400,\n",
       "  'message': 'API key not valid. Please pass a valid API key.',\n",
       "  'status': 'INVALID_ARGUMENT',\n",
       "  'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo',\n",
       "    'reason': 'API_KEY_INVALID',\n",
       "    'domain': 'googleapis.com',\n",
       "    'metadata': {'service': 'kgsearch.googleapis.com'}}]}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_knowledge_graph('Tom Hank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ae96982-6cf9-4062-8040-6080c496be28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past here Disney\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statement</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Disney continued to produce cartoons with Mickey Mouse and other characters</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Disney continued to focus its talents on television throughout the 1950s</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     Statement  \\\n",
       "0  Disney continued to produce cartoons with Mickey Mouse and other characters   \n",
       "1     Disney continued to focus its talents on television throughout the 1950s   \n",
       "\n",
       "   Count  \n",
       "0    6.3  \n",
       "1    4.3  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_in_blank_q_generate('Tom Hanks', 'Disney', facts = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7ae70f-3bc3-44bf-a199-f97bddb59468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "677efd62-03dc-4efe-8c77-eb5e39562e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bb65a1f-5377-473e-9754-9ffc2b9688f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Statement'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2938838f-c462-4328-a199-31867bca8e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resultScore</th>\n",
       "      <th>@type</th>\n",
       "      <th>result.url</th>\n",
       "      <th>result.detailedDescription.license</th>\n",
       "      <th>result.detailedDescription.url</th>\n",
       "      <th>result.detailedDescription.articleBody</th>\n",
       "      <th>result.image.contentUrl</th>\n",
       "      <th>result.image.url</th>\n",
       "      <th>result.@type</th>\n",
       "      <th>result.@id</th>\n",
       "      <th>result.name</th>\n",
       "      <th>result.description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31381.007812</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>http://disney.com</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Walt_Disney_...</td>\n",
       "      <td>The Walt Disney Company, commonly known as Dis...</td>\n",
       "      <td>https://encrypted-tbn2.gstatic.com/images?q=tb...</td>\n",
       "      <td>https://ar.m.wikipedia.org/wiki/%D9%85%D9%84%D...</td>\n",
       "      <td>[Corporation, Organization, Thing, TheaterGroup]</td>\n",
       "      <td>kg:/m/09b3v</td>\n",
       "      <td>The Walt Disney Company</td>\n",
       "      <td>Entertainment company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15199.707031</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>http://www.waltdisney.com</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Walt_Disney</td>\n",
       "      <td>Walter Elias Disney was an American entreprene...</td>\n",
       "      <td>https://encrypted-tbn1.gstatic.com/images?q=tb...</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:Walt_D...</td>\n",
       "      <td>[Thing, Person]</td>\n",
       "      <td>kg:/m/081nh</td>\n",
       "      <td>Walt Disney</td>\n",
       "      <td>American entrepreneur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9252.871094</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>http://movies.disney.com/the-lion-king</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Lion_King</td>\n",
       "      <td>The Lion King is a 1994 American animated musi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CreativeWork, Movie, Thing]</td>\n",
       "      <td>kg:/m/0m63c</td>\n",
       "      <td>The Lion King</td>\n",
       "      <td>1994 film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6824.843262</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>http://movies.disney.com/the-little-mermaid</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Little_Merma...</td>\n",
       "      <td>The Little Mermaid is a 1989 American animated...</td>\n",
       "      <td>https://encrypted-tbn2.gstatic.com/images?q=tb...</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:The_Li...</td>\n",
       "      <td>[CreativeWork, Movie, Thing]</td>\n",
       "      <td>kg:/m/01ry_x</td>\n",
       "      <td>The Little Mermaid</td>\n",
       "      <td>1989 film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6777.649902</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Hayao_Miyazaki</td>\n",
       "      <td>Hayao Miyazaki is a Japanese animator, directo...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:Hayao_...</td>\n",
       "      <td>[Thing, Person]</td>\n",
       "      <td>kg:/m/0534v</td>\n",
       "      <td>Hayao Miyazaki</td>\n",
       "      <td>Japanese animation director</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5211.307617</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Lady_and_the_Tramp</td>\n",
       "      <td>Lady and the Tramp is a 1955 American animated...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CreativeWork, Movie, Thing]</td>\n",
       "      <td>kg:/m/01q3w7</td>\n",
       "      <td>Lady and the Tramp</td>\n",
       "      <td>1955 film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5121.747070</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>http://movies.disney.com/cinderella</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cinderella_(1950...</td>\n",
       "      <td>Cinderella is a 1950 American animated musical...</td>\n",
       "      <td>https://encrypted-tbn2.gstatic.com/images?q=tb...</td>\n",
       "      <td>https://es.wikipedia.org/wiki/Archivo:1950_is_...</td>\n",
       "      <td>[CreativeWork, Movie, Thing]</td>\n",
       "      <td>kg:/m/023p33</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>1950 film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3643.347412</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>http://movies.disney.com/aladdin</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Aladdin_(1992_Di...</td>\n",
       "      <td>Aladdin is a 1992 American animated musical fa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CreativeWork, Movie, Thing]</td>\n",
       "      <td>kg:/m/0jnwx</td>\n",
       "      <td>Aladdin</td>\n",
       "      <td>1992 film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3165.555420</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>http://www.disneyanimation.com/</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Walt_Disney_Anim...</td>\n",
       "      <td>Walt Disney Animation Studios, sometimes short...</td>\n",
       "      <td>https://encrypted-tbn1.gstatic.com/images?q=tb...</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:Walt_D...</td>\n",
       "      <td>[Organization, Thing, Corporation]</td>\n",
       "      <td>kg:/m/04rcl7</td>\n",
       "      <td>Walt Disney Animation Studios</td>\n",
       "      <td>Animation company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3002.821777</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>http://movies.disney.com/</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Walt_Disney_Pict...</td>\n",
       "      <td>Walt Disney Pictures is an American film produ...</td>\n",
       "      <td>https://encrypted-tbn3.gstatic.com/images?q=tb...</td>\n",
       "      <td>https://ar.m.wikipedia.org/wiki/%D9%85%D9%84%D...</td>\n",
       "      <td>[Organization, Thing, Corporation]</td>\n",
       "      <td>kg:/m/01795t</td>\n",
       "      <td>Walt Disney Pictures</td>\n",
       "      <td>Film studio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    resultScore               @type  \\\n",
       "0  31381.007812  EntitySearchResult   \n",
       "1  15199.707031  EntitySearchResult   \n",
       "2   9252.871094  EntitySearchResult   \n",
       "3   6824.843262  EntitySearchResult   \n",
       "4   6777.649902  EntitySearchResult   \n",
       "5   5211.307617  EntitySearchResult   \n",
       "6   5121.747070  EntitySearchResult   \n",
       "7   3643.347412  EntitySearchResult   \n",
       "8   3165.555420  EntitySearchResult   \n",
       "9   3002.821777  EntitySearchResult   \n",
       "\n",
       "                                    result.url  \\\n",
       "0                            http://disney.com   \n",
       "1                    http://www.waltdisney.com   \n",
       "2       http://movies.disney.com/the-lion-king   \n",
       "3  http://movies.disney.com/the-little-mermaid   \n",
       "4                                          NaN   \n",
       "5                                          NaN   \n",
       "6          http://movies.disney.com/cinderella   \n",
       "7             http://movies.disney.com/aladdin   \n",
       "8              http://www.disneyanimation.com/   \n",
       "9                    http://movies.disney.com/   \n",
       "\n",
       "                  result.detailedDescription.license  \\\n",
       "0  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "1  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "2  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "3  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "4  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "5  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "6  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "7  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "8  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "9  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "\n",
       "                      result.detailedDescription.url  \\\n",
       "0  https://en.wikipedia.org/wiki/The_Walt_Disney_...   \n",
       "1          https://en.wikipedia.org/wiki/Walt_Disney   \n",
       "2        https://en.wikipedia.org/wiki/The_Lion_King   \n",
       "3  https://en.wikipedia.org/wiki/The_Little_Merma...   \n",
       "4       https://en.wikipedia.org/wiki/Hayao_Miyazaki   \n",
       "5   https://en.wikipedia.org/wiki/Lady_and_the_Tramp   \n",
       "6  https://en.wikipedia.org/wiki/Cinderella_(1950...   \n",
       "7  https://en.wikipedia.org/wiki/Aladdin_(1992_Di...   \n",
       "8  https://en.wikipedia.org/wiki/Walt_Disney_Anim...   \n",
       "9  https://en.wikipedia.org/wiki/Walt_Disney_Pict...   \n",
       "\n",
       "              result.detailedDescription.articleBody  \\\n",
       "0  The Walt Disney Company, commonly known as Dis...   \n",
       "1  Walter Elias Disney was an American entreprene...   \n",
       "2  The Lion King is a 1994 American animated musi...   \n",
       "3  The Little Mermaid is a 1989 American animated...   \n",
       "4  Hayao Miyazaki is a Japanese animator, directo...   \n",
       "5  Lady and the Tramp is a 1955 American animated...   \n",
       "6  Cinderella is a 1950 American animated musical...   \n",
       "7  Aladdin is a 1992 American animated musical fa...   \n",
       "8  Walt Disney Animation Studios, sometimes short...   \n",
       "9  Walt Disney Pictures is an American film produ...   \n",
       "\n",
       "                             result.image.contentUrl  \\\n",
       "0  https://encrypted-tbn2.gstatic.com/images?q=tb...   \n",
       "1  https://encrypted-tbn1.gstatic.com/images?q=tb...   \n",
       "2                                                NaN   \n",
       "3  https://encrypted-tbn2.gstatic.com/images?q=tb...   \n",
       "4  https://encrypted-tbn0.gstatic.com/images?q=tb...   \n",
       "5                                                NaN   \n",
       "6  https://encrypted-tbn2.gstatic.com/images?q=tb...   \n",
       "7                                                NaN   \n",
       "8  https://encrypted-tbn1.gstatic.com/images?q=tb...   \n",
       "9  https://encrypted-tbn3.gstatic.com/images?q=tb...   \n",
       "\n",
       "                                    result.image.url  \\\n",
       "0  https://ar.m.wikipedia.org/wiki/%D9%85%D9%84%D...   \n",
       "1  https://commons.wikimedia.org/wiki/File:Walt_D...   \n",
       "2                                                NaN   \n",
       "3  https://commons.wikimedia.org/wiki/File:The_Li...   \n",
       "4  https://commons.wikimedia.org/wiki/File:Hayao_...   \n",
       "5                                                NaN   \n",
       "6  https://es.wikipedia.org/wiki/Archivo:1950_is_...   \n",
       "7                                                NaN   \n",
       "8  https://commons.wikimedia.org/wiki/File:Walt_D...   \n",
       "9  https://ar.m.wikipedia.org/wiki/%D9%85%D9%84%D...   \n",
       "\n",
       "                                       result.@type    result.@id  \\\n",
       "0  [Corporation, Organization, Thing, TheaterGroup]   kg:/m/09b3v   \n",
       "1                                   [Thing, Person]   kg:/m/081nh   \n",
       "2                      [CreativeWork, Movie, Thing]   kg:/m/0m63c   \n",
       "3                      [CreativeWork, Movie, Thing]  kg:/m/01ry_x   \n",
       "4                                   [Thing, Person]   kg:/m/0534v   \n",
       "5                      [CreativeWork, Movie, Thing]  kg:/m/01q3w7   \n",
       "6                      [CreativeWork, Movie, Thing]  kg:/m/023p33   \n",
       "7                      [CreativeWork, Movie, Thing]   kg:/m/0jnwx   \n",
       "8                [Organization, Thing, Corporation]  kg:/m/04rcl7   \n",
       "9                [Organization, Thing, Corporation]  kg:/m/01795t   \n",
       "\n",
       "                     result.name           result.description  \n",
       "0        The Walt Disney Company        Entertainment company  \n",
       "1                    Walt Disney        American entrepreneur  \n",
       "2                  The Lion King                    1994 film  \n",
       "3             The Little Mermaid                    1989 film  \n",
       "4                 Hayao Miyazaki  Japanese animation director  \n",
       "5             Lady and the Tramp                    1955 film  \n",
       "6                     Cinderella                    1950 film  \n",
       "7                        Aladdin                    1992 film  \n",
       "8  Walt Disney Animation Studios            Animation company  \n",
       "9           Walt Disney Pictures                  Film studio  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_knowledge_graph_df('Walt Disney')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cccb2a-6983-410c-b743-c1913984c6f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
