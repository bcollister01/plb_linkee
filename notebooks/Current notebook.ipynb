{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f1f800c-9004-4662-a84e-6b9e58cdc9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#!pip install wikipedia\n",
    "#!pip install yake\n",
    "#!pip install --upgrade ecommercetools\n",
    "#!pip install pattern\n",
    "#!pip install textacy\n",
    "\n",
    "# Import packages\n",
    "import wikipedia\n",
    "import re\n",
    "import yake\n",
    "import nltk #For some reason, had to uninstall and reinstall\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ecommercetools import seo\n",
    "#from pattern.text.en import singularize, pluralize #issues here\n",
    "#For Google Knowledge Graph API\n",
    "import requests\n",
    "import urllib\n",
    "import json\n",
    "import os\n",
    "from pattern.text.en import singularize, pluralize\n",
    "from requests_html import HTML\n",
    "from requests_html import HTMLSession\n",
    "#if scraping paragraphs from first few webpages\n",
    "from bs4 import BeautifulSoup\n",
    "#For question generation\n",
    "import spacy\n",
    "import textacy\n",
    "#The commented lines below only need run the first time\n",
    "# !pip install git+https://github.com/uob-vil/pattern.git\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('omw-1.4')\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "pd.set_option('max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a01ed597-7e02-4ed4-a8a1-1d13079471de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ending_pluralize(noun):\n",
    "    '''Return most appropriate plural of the input word.'''\n",
    "    if re.search('[sxz]$', noun):\n",
    "        return re.sub('$', 'es', noun)\n",
    "    elif re.search('[^aeioudgkprt]h$', noun):\n",
    "        return re.sub('$', 'es', noun)\n",
    "    elif re.search('y$', noun):\n",
    "        return re.sub('y$', 'ies', noun)\n",
    "    else:\n",
    "        return noun\n",
    "\n",
    "def add_s_pluralize(noun):\n",
    "    '''Naively add s to end of input word to create plural'''\n",
    "    return noun + 's'\n",
    "  \n",
    "def tidy_input(input):\n",
    "    '''Take input word and tidy it up to create a list of options.\n",
    "    \n",
    "    We have a few different pluralize functions just to account for any\n",
    "    misspellings online/words created when punctuation removed.\n",
    "    '''\n",
    "  \n",
    "    input_words = input.split()\n",
    "  \n",
    "    #Add singular forms of plurals and plural forms of singles \n",
    "    singles = [singularize(plural) for plural in input_words]\n",
    "    plurals1 = [pluralize(single) for single in singles]\n",
    "    plurals2 = [ending_pluralize(single) for single in singles]\n",
    "    plurals3 = [add_s_pluralize(single) for single in singles]\n",
    "    input_words = input_words + singles + plurals1 + plurals2 + plurals3\n",
    "  \n",
    "    input_words = input_words + [word.lower() for word in input_words]\n",
    "    #If you want capitalized words as well\n",
    "    input_words = input_words + [word[0].upper() + word[1:] for word in input.split()]\n",
    "    input_words = input_words + [word.upper() for word in input_words]\n",
    "  \n",
    "    input_words = list(set(input_words))\n",
    "    \n",
    "    return(input_words)\n",
    "  \n",
    "#Next few functions sourced from https://practicaldatascience.co.uk/data-science/how-to-access-the-google-knowledge-graph-search-api\n",
    "  \n",
    "def get_source(url):\n",
    "    \"\"\"Returns the source code for the provided URL. \n",
    "  \n",
    "    Parameters\n",
    "      ----------\n",
    "    url (string): URL of the page to scrape.\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    response (object): HTTP response object from requests_html. \n",
    "    \"\"\"\n",
    "  \n",
    "    try:\n",
    "        session = HTMLSession()\n",
    "        response = session.get(url)\n",
    "        return response\n",
    "  \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "\n",
    "def get_knowledge_graph(api_key, query):\n",
    "    \"\"\"Return a Google Knowledge Graph for a given query.\n",
    "\n",
    "    Parameters\n",
    "    ---------- \n",
    "    api_key (string): Google Knowledge Graph API key. \n",
    "    query (string): Term to search for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    response (object): Knowledge Graph response object in JSON format.\n",
    "    \"\"\" \n",
    "\n",
    "    endpoint = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
    "    params = {\n",
    "      'query': query,\n",
    "      'limit': 10,\n",
    "      'indent': True,\n",
    "      'key': api_key,\n",
    "    }\n",
    "\n",
    "    url = endpoint + '?' + urllib.parse.urlencode(params)    \n",
    "    response = get_source(url)\n",
    "\n",
    "    return json.loads(response.text)\n",
    "  \n",
    "def get_knowledge_graph_df(input):\n",
    "    \"\"\"\n",
    "    Uses Google's knowledge graph to generate Pandas DataFrame of entities \n",
    "    deemed most similar to input searched. DataFrame includes categorization\n",
    "    of entity, title, short description and URL (usually to Wikipedia).\n",
    "    You will need to have set up an API key in Google Cloud Console to get this\n",
    "    to work (it's free to do and you can do 100k requests a day I believe.)\n",
    "    https://console.cloud.google.com/apis \n",
    "    Parameters\n",
    "    ----------\n",
    "    input (string): Final Linkee answer\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    knowledge_graph_df (Pandas DataFrame): info on Knowledge Graph results\n",
    "    \"\"\"\n",
    "    threshold=0.2\n",
    "    api_key = os.environ['GOOGLE_LINKEE_KEY']\n",
    "    knowledge_graph_json = get_knowledge_graph(api_key, input)\n",
    "    knowledge_graph_df = pd.json_normalize(knowledge_graph_json, record_path='itemListElement')\n",
    "    return knowledge_graph_df\n",
    "    #Only using scores if knowledge graph actually returns something\n",
    "    if len(knowledge_graph_df) > 0:\n",
    "        max_score = max(knowledge_graph_df['resultScore'])\n",
    "        knowledge_graph_df = knowledge_graph_df.loc[knowledge_graph_df['resultScore']>threshold*max_score]\n",
    "        index_match = knowledge_graph_df.index[knowledge_graph_df['result.name'] == input]\n",
    "        if len(index_match) == 1:\n",
    "            n = index_match[0]\n",
    "            knowledge_graph_df = pd.concat([knowledge_graph_df.iloc[[n],:], knowledge_graph_df.drop(n, axis=0)], axis=0)\n",
    "            knowledge_graph_df.reset_index(inplace = True, drop = True )\n",
    "    return knowledge_graph_df\n",
    "  \n",
    "def classify_input(knowledge_graph_df):\n",
    "    \"\"\"Classify the input word/phrase as a certain category \n",
    "    to improve search results. Acts as failsafe if initial search\n",
    "    of input fails.\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    knowledge_graph_df: Return of get_knowledge_graph_df\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    category (string): Category of input\n",
    "    \n",
    "    \"\"\"\n",
    "    if \"SportsTeam\" in knowledge_graph_df['result.@type'][0]:\n",
    "        entity_tags = knowledge_graph_df['result.@type'][1]\n",
    "    else:\n",
    "        entity_tags = knowledge_graph_df['result.@type'][0]\n",
    "    \n",
    "    if (\"Movie\" in entity_tags) or (\"MovieSeries\" in entity_tags):\n",
    "        category = \"Movie\"\n",
    "    elif (\"TVEpisode\" in entity_tags) or (\"TVSeries\" in entity_tags):\n",
    "        category = \"TV\"\n",
    "    elif (\"VideoGame\" in entity_tags) or (\"VideoGameSeries\" in entity_tags):\n",
    "        category = \"VideoGame\"\n",
    "    elif (\"Book\" in entity_tags) or (\"BookSeries\" in entity_tags):\n",
    "        category = \"Book\"\n",
    "    elif \"Person\" in entity_tags:\n",
    "        category = \"Person\"\n",
    "    elif (\"MusicAlbum\" in entity_tags) or (\"MusicGroup\" in entity_tags) or (\"MusicRecording\" in entity_tags):\n",
    "        category = \"Music\"\n",
    "    elif (\"Place\" in entity_tags) or (\"AdministrativeArea\" in entity_tags):\n",
    "        category = \"Place\" \n",
    "    else:\n",
    "        category = \"Thing\"\n",
    "  \n",
    "    return(category)\n",
    "  \n",
    "def tailored_search(category, input):\n",
    "    \"\"\"Change the search to get better keywords for input, based on its category\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    category (string): Category of input\n",
    "    input (string): Final Linkee answer\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    search_input (string): Search term to use to find keywords\n",
    "    \n",
    "    \"\"\"\n",
    "    if category == \"Movie\" or category == \"TV\" or category == \"Book\":\n",
    "        search_input = input + \" \" + category + \" information\"\n",
    "    elif category == \"Place\":\n",
    "        search_input = input + \" location\"\n",
    "    else:\n",
    "        search_input = input\n",
    "    return(search_input)\n",
    "  \n",
    "def collect_urls(knowledge_graph_df):\n",
    "    \"\"\"Collect the urls from the knowledge graph to give more options to scrape \n",
    "    from.\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    knowledge_graph_df: Return of get_knowledge_graph_df\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    list of urls (string): urls found\n",
    "    \n",
    "    \"\"\"\n",
    "    if 'result.detailedDescription.url' in knowledge_graph_df.columns:\n",
    "        knowledge_graph_df = knowledge_graph_df[knowledge_graph_df['result.detailedDescription.url'].notna()]\n",
    "        urlList = knowledge_graph_df['result.detailedDescription.url'].tolist()\n",
    "    else:\n",
    "        urlList = []\n",
    "    return urlList\n",
    "\n",
    "def get_wiki_links(urlList):\n",
    "    '''Extract the URLs linking to Wikipedia from a list of URLs'''\n",
    "    url_wiki=[urlList[i] for i in range(len(urlList)) if urlList[i].find(\"wiki\")!= -1]\n",
    "    # if len(url_wiki) == 0:\n",
    "    #   print('No Urls')\n",
    "    # if url_wiki:\n",
    "    return(url_wiki)\n",
    "  \n",
    "def get_wiki_text(url_wiki, keep_words=10000):\n",
    "    '''\n",
    "    Takes a list of urls and scrapes from Wikipedia links\n",
    "    if present.\n",
    "    Parameters\n",
    "    ----------\n",
    "    urlList (list) : a list of wikipedia urls\n",
    "    keep_words (integer): the number of words to keep (approx up to paragraph)\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    text_comb (string): the text extracted from the paragraphs until word limit \n",
    "                          reached\n",
    "    '''\n",
    "    text_comb = ''\n",
    "    total_words = 0\n",
    "    key_url_terms  = []\n",
    "    if len(url_wiki)> 1:\n",
    "        url_terms = [url.split('/wiki/')[1] for url in url_wiki[1:]] \n",
    "        # limiting to 3 max by the fact the url should be limited like that anyway\n",
    "        for s in url_terms:\n",
    "            s = s.replace(\"_\",\" \")\n",
    "            key_term = s.translate(str.maketrans('', '', string.punctuation))\n",
    "            key_term = \" \".join(key_term.split())\n",
    "            key_url_terms.append(key_term)\n",
    "            # print('wiki url', key_url_terms)\n",
    "        # print('url_wiki',url_wiki)\n",
    "    for url in url_wiki:\n",
    "        wiki_term = url.split('/wiki/')[1]\n",
    "        print(f\"Looking at wiki page for: {wiki_term}\")\n",
    "        try:\n",
    "            text_wiki = (wikipedia.page(wiki_term, auto_suggest = False)).content\n",
    "        except KeyError: #fullurl errors can be caused by unicode or other symbols\n",
    "            text_wiki = (wikipedia.page(wiki_term, auto_suggest = True)).content\n",
    "        #This will drop headers surrounded by ==\n",
    "        text_wiki = re.sub(r'==.*?==+', '', text_wiki)\n",
    "        paras = text_wiki.split('\\n\\n')\n",
    "        word_count = len(paras[0].split()) #number of words in 1st paragraph\n",
    "        remaining_words = keep_words - total_words\n",
    "        j = 0\n",
    "        text = paras[0]\n",
    "        while word_count < remaining_words and j<len(paras)-1:\n",
    "            j += 1\n",
    "            para_text = paras[j]\n",
    "            word_count = word_count + len(para_text.split())\n",
    "            text = text + ' ' + para_text\n",
    "        #Drop new line /n clutter\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = re.sub(\"\\s\\s+\", \" \", text)\n",
    "        text_comb = text_comb + text # change if want more than one\n",
    "        total_words = total_words + word_count\n",
    "        if total_words >= keep_words:\n",
    "            break  # break out of for loop when we have enough words\n",
    "\n",
    "    return text_comb, key_url_terms\n",
    "  \n",
    "def wiki_autosuggest(input, keep_words = 10000, suggest = False):\n",
    "    ''' \n",
    "    Gets text from Wikipedia using whichever page is found and cleans up the text\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    input (string): original input word\n",
    "    keep_words (integer): number of words to keep\n",
    "    suggest (Boolean): suggest = True means use wikipedia autosuggest function,\n",
    "                       False takes the input as is to find the page\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    text (string): text that has been cleaned up\n",
    "               \n",
    "    '''\n",
    "    # Get text from single wikipedia page\n",
    "    try:\n",
    "        text_wiki = (wikipedia.page(input, auto_suggest = suggest)).content\n",
    "    except Exception as err:\n",
    "        print(err.args)\n",
    "        raise ValueError(f'No urls found for {input}') \n",
    "    #if no exception raised clean up text\n",
    "    #This will drop headers surrounded by ==\n",
    "    text_wiki = re.sub(r'==.*?==+', '', text_wiki)\n",
    "    paras = text_wiki.split('\\n\\n')\n",
    "    word_count=len(paras[0].split()) #number of words in 1st paragraph\n",
    "    j=0\n",
    "    text = paras[0]\n",
    "    while word_count < keep_words and j<len(paras)-1:\n",
    "        j += 1\n",
    "        para_text = paras[j]\n",
    "        word_count = word_count + len(para_text.split())\n",
    "        text = text + ' ' + para_text\n",
    "    #Drop new line /n clutter\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub(\"\\s\\s+\", \" \", text)\n",
    "    return text\n",
    "  \n",
    "def find_text(input, keep_words=10000, cleanup=True, multi_links = True): \n",
    "    '''  \n",
    "    Finds text related to input that can be used for keyword extraction. \n",
    "    The text is found in the following order, using wikipedia directly without \n",
    "    autosuggest (as autosuggest can sometimes have a weird error, e.g Belfast),\n",
    "    wikipedia using autosuggest, then tries using knowledge graph to find wiki links\n",
    "    This function attempts to clean up the relevant text if cleanup is set to True.\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "  \n",
    "    input (string): input word (final answer in Linkee)\n",
    "    keep_words (integer): the number of words (+to end of paragraph) to keep in text.\n",
    "    multi_links (Boolean): flag for using more than one wiki page\n",
    "    text (string): the block of text extracted from wiki\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "  \n",
    "    key_url_terms (list): the page names of any pages used to add to keywords\n",
    "    '''\n",
    "    key_url_terms = []\n",
    "    try:\n",
    "        text = wiki_autosuggest(input, keep_words = keep_words, suggest = False)\n",
    "    except Exception as ex:\n",
    "        print(f\"failed with {ex}\")\n",
    "        try:\n",
    "            text = wiki_autosuggest(input, keep_words = keep_words, suggest = True)\n",
    "        except Exception as e:\n",
    "            print(f\"failed with {e}\")\n",
    "            knowledge_graph_df = get_knowledge_graph_df(input)\n",
    "            if len(knowledge_graph_df) == 0:\n",
    "                print(\"No valid keyword\")\n",
    "                # print(\"nothing found using knowledge graph, trying wiki\")\n",
    "                return \"No valid keyword\", []\n",
    "                #keyword vs \n",
    "            else:\n",
    "                # Try to get wiki pages from knowledge graph\n",
    "                urlList = collect_urls(knowledge_graph_df)\n",
    "                url_wiki = get_wiki_links(urlList)\n",
    "                if len(url_wiki) >= 1:\n",
    "                    keep = min(len(url_wiki), 3)\n",
    "                    url_wiki = url_wiki[0:keep]\n",
    "                    if multi_links == False:\n",
    "                        url_wiki = url_wiki[0:1]\n",
    "                        text, key_url_terms = get_wiki_text(url_wiki, keep_words)\n",
    "                else: \n",
    "                    #Use the knowledge graph categories to find wikipedia url\n",
    "                    print(\" No wiki urls: 1st pass\")\n",
    "                    category = classify_input(knowledge_graph_df)\n",
    "                    search_input = tailored_search(category, input)\n",
    "                    print(f\"Searching for urls with input {search_input}\")\n",
    "                    urlList = collect_urls(get_knowledge_graph_df(search_input))\n",
    "                    url_wiki = get_wiki_links(urlList)\n",
    "                    if len(url_wiki) >= 1:\n",
    "                        keep = min(len(url_wiki), 3)\n",
    "                        url_wiki = url_wiki[0:keep]\n",
    "                        if multi_links == False:\n",
    "                            url_wiki = url_wiki[0:1]\n",
    "                        text, key_url_terms  = get_wiki_text(url_wiki, keep_words)\n",
    "                    else:\n",
    "                        print(\"no wiki pages found\")\n",
    "                        return \"No valid keyword\", []\n",
    "                        # print(\" No wiki urls: 2nd pass\")\n",
    "                        # text = wiki_autosuggest(input)\n",
    "    \n",
    "  \n",
    "    #Text Cleaning\n",
    "    text = re.sub(r\"\\'\", '', text) #Get rid of \\'\n",
    "    text = re.sub(r\"\\\\xa0...\", '', text) #Get rid of \\\\xa0...\n",
    "    text = re.sub(r\"\\\\n\", ' ', text) #Get rid of \\\\n\n",
    "    text = re.sub(r\"\\\\u200e\", ' ', text) #Get rid of \\\\u200e\n",
    "    text = re.sub(r\"U S \", \"US \", text)\n",
    "    text = re.sub(r\"logo\", '', text)\n",
    "    text = re.sub(r\"[Vv]iew \\d+ more rows\", '', text) #Get rid of [Vv]iew \\d+ more rows\n",
    "    text = re.sub(r\"\\d+ hours ago\", '', text)\n",
    "    #Remove things like \"2009.Power\" - no space after full stop\n",
    "    rx = r\"\\.(?=[A-Za-z])\"\n",
    "    text = re.sub(rx, \". \", text)\n",
    "    if cleanup == True:\n",
    "        text = re.sub(r\"[\\\"\\'\\“\\”\\[\\]\\)\\(\\•\\▽\\❖\\†]+\", '', text)\n",
    "        text = re.sub(r\"[-·—,.;:@#?!$+-]+\", ' ', text) \n",
    "    \n",
    "    text = ' '.join(text.split()) #Single spacing\n",
    "  \n",
    "    return text, key_url_terms\n",
    "\n",
    "def keyword_extract(text, ngram_size):\n",
    "    '''Extract keywords/phrases of ngram_size using YAKE'''\n",
    "    #Initialise extractor\n",
    "    kw_extractor = yake.KeywordExtractor()\n",
    "    language = \"en\"\n",
    "    max_ngram_size = ngram_size\n",
    "    deduplication_threshold = 0.3\n",
    "    numOfKeywords = 100\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, \n",
    "                                                n=max_ngram_size, \n",
    "                                                dedupLim=deduplication_threshold, \n",
    "                                                top=numOfKeywords, features=None)\n",
    "    \n",
    "    #Run extractor on text and get out words/phrases\n",
    "    yake_output = custom_kw_extractor.extract_keywords(text)\n",
    "    words, scores = zip(*yake_output)\n",
    "    words = list(words)\n",
    "    scores = list(scores)\n",
    "    words = [re.sub(r\"[,.;@#?!$]+\", ' ', i) for i in words]\n",
    "    return(words,scores)\n",
    "  \n",
    "def answer_keyword_compare(keywords_list, input_words):\n",
    "    '''Remove candidate keywords that contain input words'''\n",
    "  \n",
    "    keywords_list = [x for x in keywords_list if not any(i in input_words for i in x.split())]\n",
    "    return keywords_list\n",
    "  \n",
    "def remove_non_noun_full_keywords(keywords_list):\n",
    "    '''\n",
    "    Only retain keywords/keyphrases that are proper nouns.\n",
    "    '''\n",
    "    pos = nltk.pos_tag(keywords_list)\n",
    "    new_keyword_list = []\n",
    "    for ii in np.arange(0,len(pos),1):\n",
    "        if pos[ii][1]=='NNP':\n",
    "            new_keyword_list.append(pos[ii][0])\n",
    "        if pos[ii][1]=='NNPS':\n",
    "            new_keyword_list.append(pos[ii][0])\n",
    "    return new_keyword_list\n",
    "  \n",
    "def select_keywords(words2):\n",
    "    '''\n",
    "    Selects the keywords/phrases to use for question generation. Ensures that \n",
    "    keyword phrases do not overlap each other.\n",
    "    '''\n",
    "    words3 = []\n",
    "    words3.append(words2[0])\n",
    "    del words2[0]\n",
    "    for i in range(len(words2)):\n",
    "        #If at any point, we only have 4 candidate keywords left, use them all\n",
    "        if len(words2) + len(words3) <= 4:\n",
    "            words3 = words3 + words2\n",
    "            break\n",
    "        test_words = words2[0].lower().split()\n",
    "        singles = [singularize(plural) for plural in test_words]\n",
    "        plurals1 = [pluralize(single) for single in singles]\n",
    "        plurals2 = [ending_pluralize(single) for single in singles]\n",
    "        plurals3 = [add_s_pluralize(single) for single in singles]\n",
    "        test_words = list(set(test_words + singles + plurals1 + plurals2 + plurals3))\n",
    "        previous_words = words3.copy()\n",
    "        previous_words = [word for phrase in previous_words for word in phrase.split()]\n",
    "        previous_words = [x.lower() for x in previous_words]\n",
    "        if len(test_words) + len(previous_words) == len(list(set(test_words + previous_words))):\n",
    "            words3.append(words2[0])\n",
    "        del words2[0]\n",
    "    return(words3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "762ec0d2-bbf4-4839-801c-1b0802e97958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resultScore</th>\n",
       "      <th>@type</th>\n",
       "      <th>result.@id</th>\n",
       "      <th>result.image.url</th>\n",
       "      <th>result.image.contentUrl</th>\n",
       "      <th>result.name</th>\n",
       "      <th>result.url</th>\n",
       "      <th>result.description</th>\n",
       "      <th>result.detailedDescription.url</th>\n",
       "      <th>result.detailedDescription.license</th>\n",
       "      <th>result.detailedDescription.articleBody</th>\n",
       "      <th>result.@type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4592.872070</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>kg:/m/01l3vx</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:France_national_football_team_2018.jpg</td>\n",
       "      <td>https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcQAdbRZZPTHrhz24xj3Xdzf6RGl3MOdTQJFwoBfz8eI0mFJYhiJ</td>\n",
       "      <td>France national football team</td>\n",
       "      <td>http://www.fff.fr/bleus/</td>\n",
       "      <td>Football team</td>\n",
       "      <td>https://en.wikipedia.org/wiki/France_national_football_team</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License</td>\n",
       "      <td>The France national football team represents France in men's international football and is controlled by the French Football Federation, also known as FFF. The team's colours are blue, white, and red, and the coq gaulois its symbol. France are colloquially known as Les Bleus. They are the reigning world champions, having won the most recent World Cup final in 2018.\\n</td>\n",
       "      <td>[Thing, SportsTeam]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2954.216553</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>kg:/m/044hxl</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:Ligue1.svg</td>\n",
       "      <td>https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcSP_jf42o1P3eB4PidA5AV6ZurulDZyEUV5ES6xLOCqB3_1dArr</td>\n",
       "      <td>Ligue 1</td>\n",
       "      <td>http://www.ligue1.com/</td>\n",
       "      <td>Football league</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Ligue_1</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License</td>\n",
       "      <td>Ligue 1, officially known as Ligue 1 Uber Eats for sponsorship reasons, is a French professional league for men's association football clubs. At the top of the French football league system, it is the country's primary football competition.</td>\n",
       "      <td>[SportsOrganization, Organization, Corporation, Thing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2823.790039</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>kg:/m/04y8tkw</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:Leroy_Merlin.svg</td>\n",
       "      <td>https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcTBzsi3jT4FGBK_CDRu9ysOk4Nfm7XETAE_cSPRsdmsITXcLkLL</td>\n",
       "      <td>Leroy Merlin</td>\n",
       "      <td>http://www.leroymerlin.com/</td>\n",
       "      <td>Retail company</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Leroy_Merlin</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License</td>\n",
       "      <td>Leroy Merlin is a French headquartered home improvement and gardening retailer serving several countries in Europe, Asia, South America, and Africa.</td>\n",
       "      <td>[Organization, Thing, Corporation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2259.376709</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>kg:/m/0jd05</td>\n",
       "      <td>https://fr.m.wikipedia.org/wiki/Fichier:Orange_logo.svg</td>\n",
       "      <td>https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcRkC7L1ooTL5B_MXf04rHuYIGcVJk_H2WzBCUS84PxC96qgSbpM</td>\n",
       "      <td>Orange S.A.</td>\n",
       "      <td>http://www.orange.com</td>\n",
       "      <td>Telecom company</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Orange_S.A.</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License</td>\n",
       "      <td>Orange S.A., rebranded as Orange, formerly France Télécom S.A., stylized as france telecom, is a French multinational telecommunications corporation. It has 266 million customers worldwide and employs 89,000 people in France, and 59,000 elsewhere.</td>\n",
       "      <td>[Organization, Thing, Corporation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2098.627930</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>kg:/m/0h7k5</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:Air_France_logo_(1970%27s-2008).svg</td>\n",
       "      <td>https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcSkQIkHn6SO7PF9BTu-WG-YZK8Q8XsGbuyY0U3yN7waKPInlIbt</td>\n",
       "      <td>Air France</td>\n",
       "      <td>http://www.airfrance.com</td>\n",
       "      <td>Air carrier</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Air_France</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License</td>\n",
       "      <td>Air France, stylised as AIRFRANCE, is the flag carrier of France headquartered in Tremblay-en-France. It is a subsidiary of the Air France–KLM Group and a founding member of the SkyTeam global airline alliance.</td>\n",
       "      <td>[Organization, Corporation, Thing, Airline]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1986.733521</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>kg:/m/028vbm</td>\n",
       "      <td>https://fr.m.wikipedia.org/wiki/Fichier:Caa-com_rvb.png</td>\n",
       "      <td>https://encrypted-tbn2.gstatic.com/images?q=tbn:ANd9GcRVAwirPPbMKnvDJTO-iSGkX9MDdTbWrW3vT5Exg2S9s0aMgW_l</td>\n",
       "      <td>Crédit Agricole</td>\n",
       "      <td>http://www.credit-agricole.com/</td>\n",
       "      <td>Bank</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cr%C3%A9dit_Agricole</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License</td>\n",
       "      <td>Crédit Agricole Group, sometimes called La banque verte due to its historical ties to farming, is a French international banking group and the world's largest cooperative financial institution.</td>\n",
       "      <td>[Organization, Thing, Corporation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1700.724854</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>kg:/g/122044ns</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:Logo_Maisons_du_Monde_FR.png</td>\n",
       "      <td>https://encrypted-tbn2.gstatic.com/images?q=tbn:ANd9GcTIfaTLFxMfJDJ2fQV6aHV8T9ntL7y2M9cPG2k2kGbaB6WvSNYM</td>\n",
       "      <td>Maisons du Monde</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Company</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Maisons_du_Monde</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License</td>\n",
       "      <td>Maisons du Monde is a French furniture and home decor company founded in Brest in 1996 by Xavier Marie. At the end of 2015 it had nearly 250 stores across France, Italy, Spain, Luxembourg, Belgium, Germany and in Switzerland, of which more than 180 are in France.</td>\n",
       "      <td>[Organization, Thing, Corporation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1591.487793</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>kg:/m/03y4ty</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Électricité de France</td>\n",
       "      <td>http://www.edf.com</td>\n",
       "      <td>Nuclear electric power generation company</td>\n",
       "      <td>https://en.wikipedia.org/wiki/%C3%89lectricit%C3%A9_de_France</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License</td>\n",
       "      <td>Électricité de France S.A., commonly known as EDF, is a French multinational electric utility company, largely owned by the French state.</td>\n",
       "      <td>[Organization, Thing, Corporation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1367.850952</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>kg:/m/054yn8</td>\n",
       "      <td>https://pl.wikipedia.org/wiki/Plik:G%C5%82og%C3%B3w-Castorama.jpg</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRw4QkUYiO2XK6Ljq3jR2FQyeds78fiUsIkPhSBJtXhMV9gLb_d</td>\n",
       "      <td>Castorama</td>\n",
       "      <td>http://www.castorama.fr/store/</td>\n",
       "      <td>Retail company</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Castorama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License</td>\n",
       "      <td>Castorama is a French retailer of DIY and home improvement tools and supplies, headquartered in Templemars, France, and is part of the British group Kingfisher plc, which has 101 stores in France and 90 in Poland.</td>\n",
       "      <td>[Organization, Place, Thing, Corporation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1352.480469</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>kg:/g/11nxplyjt9</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Miss_France_2022</td>\n",
       "      <td>https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcTZsq_7Ew1owg3tGnlSyoieYAhqU1YDeI3awloPpuzZUWRoZyw8</td>\n",
       "      <td>Miss France 2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Competition</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Miss_France_2022</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License</td>\n",
       "      <td>Miss France 2022 was the 92nd edition of the Miss France pageant. The competition was held on 11 December 2021 at the Zénith de Caen in Caen, Normandy.</td>\n",
       "      <td>[TouristAttraction, Event, Thing]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   resultScore               @type        result.@id  \\\n",
       "0  4592.872070  EntitySearchResult      kg:/m/01l3vx   \n",
       "1  2954.216553  EntitySearchResult      kg:/m/044hxl   \n",
       "2  2823.790039  EntitySearchResult     kg:/m/04y8tkw   \n",
       "3  2259.376709  EntitySearchResult       kg:/m/0jd05   \n",
       "4  2098.627930  EntitySearchResult       kg:/m/0h7k5   \n",
       "5  1986.733521  EntitySearchResult      kg:/m/028vbm   \n",
       "6  1700.724854  EntitySearchResult    kg:/g/122044ns   \n",
       "7  1591.487793  EntitySearchResult      kg:/m/03y4ty   \n",
       "8  1367.850952  EntitySearchResult      kg:/m/054yn8   \n",
       "9  1352.480469  EntitySearchResult  kg:/g/11nxplyjt9   \n",
       "\n",
       "                                                                 result.image.url  \\\n",
       "0  https://commons.wikimedia.org/wiki/File:France_national_football_team_2018.jpg   \n",
       "1                              https://commons.wikimedia.org/wiki/File:Ligue1.svg   \n",
       "2                        https://commons.wikimedia.org/wiki/File:Leroy_Merlin.svg   \n",
       "3                         https://fr.m.wikipedia.org/wiki/Fichier:Orange_logo.svg   \n",
       "4     https://commons.wikimedia.org/wiki/File:Air_France_logo_(1970%27s-2008).svg   \n",
       "5                         https://fr.m.wikipedia.org/wiki/Fichier:Caa-com_rvb.png   \n",
       "6            https://commons.wikimedia.org/wiki/File:Logo_Maisons_du_Monde_FR.png   \n",
       "7                                                                             NaN   \n",
       "8               https://pl.wikipedia.org/wiki/Plik:G%C5%82og%C3%B3w-Castorama.jpg   \n",
       "9                                  https://en.wikipedia.org/wiki/Miss_France_2022   \n",
       "\n",
       "                                                                                    result.image.contentUrl  \\\n",
       "0  https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcQAdbRZZPTHrhz24xj3Xdzf6RGl3MOdTQJFwoBfz8eI0mFJYhiJ   \n",
       "1  https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcSP_jf42o1P3eB4PidA5AV6ZurulDZyEUV5ES6xLOCqB3_1dArr   \n",
       "2  https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcTBzsi3jT4FGBK_CDRu9ysOk4Nfm7XETAE_cSPRsdmsITXcLkLL   \n",
       "3  https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcRkC7L1ooTL5B_MXf04rHuYIGcVJk_H2WzBCUS84PxC96qgSbpM   \n",
       "4  https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcSkQIkHn6SO7PF9BTu-WG-YZK8Q8XsGbuyY0U3yN7waKPInlIbt   \n",
       "5  https://encrypted-tbn2.gstatic.com/images?q=tbn:ANd9GcRVAwirPPbMKnvDJTO-iSGkX9MDdTbWrW3vT5Exg2S9s0aMgW_l   \n",
       "6  https://encrypted-tbn2.gstatic.com/images?q=tbn:ANd9GcTIfaTLFxMfJDJ2fQV6aHV8T9ntL7y2M9cPG2k2kGbaB6WvSNYM   \n",
       "7                                                                                                       NaN   \n",
       "8  https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRw4QkUYiO2XK6Ljq3jR2FQyeds78fiUsIkPhSBJtXhMV9gLb_d   \n",
       "9  https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcTZsq_7Ew1owg3tGnlSyoieYAhqU1YDeI3awloPpuzZUWRoZyw8   \n",
       "\n",
       "                     result.name                       result.url  \\\n",
       "0  France national football team         http://www.fff.fr/bleus/   \n",
       "1                        Ligue 1           http://www.ligue1.com/   \n",
       "2                   Leroy Merlin      http://www.leroymerlin.com/   \n",
       "3                    Orange S.A.            http://www.orange.com   \n",
       "4                     Air France         http://www.airfrance.com   \n",
       "5                Crédit Agricole  http://www.credit-agricole.com/   \n",
       "6               Maisons du Monde                              NaN   \n",
       "7          Électricité de France               http://www.edf.com   \n",
       "8                      Castorama   http://www.castorama.fr/store/   \n",
       "9               Miss France 2022                              NaN   \n",
       "\n",
       "                          result.description  \\\n",
       "0                              Football team   \n",
       "1                            Football league   \n",
       "2                             Retail company   \n",
       "3                            Telecom company   \n",
       "4                                Air carrier   \n",
       "5                                       Bank   \n",
       "6                                    Company   \n",
       "7  Nuclear electric power generation company   \n",
       "8                             Retail company   \n",
       "9                                Competition   \n",
       "\n",
       "                                  result.detailedDescription.url  \\\n",
       "0    https://en.wikipedia.org/wiki/France_national_football_team   \n",
       "1                          https://en.wikipedia.org/wiki/Ligue_1   \n",
       "2                     https://en.wikipedia.org/wiki/Leroy_Merlin   \n",
       "3                      https://en.wikipedia.org/wiki/Orange_S.A.   \n",
       "4                       https://en.wikipedia.org/wiki/Air_France   \n",
       "5             https://en.wikipedia.org/wiki/Cr%C3%A9dit_Agricole   \n",
       "6                 https://en.wikipedia.org/wiki/Maisons_du_Monde   \n",
       "7  https://en.wikipedia.org/wiki/%C3%89lectricit%C3%A9_de_France   \n",
       "8                        https://en.wikipedia.org/wiki/Castorama   \n",
       "9                 https://en.wikipedia.org/wiki/Miss_France_2022   \n",
       "\n",
       "                                                                             result.detailedDescription.license  \\\n",
       "0  https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License   \n",
       "1  https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License   \n",
       "2  https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License   \n",
       "3  https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License   \n",
       "4  https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License   \n",
       "5  https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License   \n",
       "6  https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License   \n",
       "7  https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License   \n",
       "8  https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License   \n",
       "9  https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                              result.detailedDescription.articleBody  \\\n",
       "0  The France national football team represents France in men's international football and is controlled by the French Football Federation, also known as FFF. The team's colours are blue, white, and red, and the coq gaulois its symbol. France are colloquially known as Les Bleus. They are the reigning world champions, having won the most recent World Cup final in 2018.\\n   \n",
       "1                                                                                                                                  Ligue 1, officially known as Ligue 1 Uber Eats for sponsorship reasons, is a French professional league for men's association football clubs. At the top of the French football league system, it is the country's primary football competition.    \n",
       "2                                                                                                                                                                                                                              Leroy Merlin is a French headquartered home improvement and gardening retailer serving several countries in Europe, Asia, South America, and Africa.    \n",
       "3                                                                                                                           Orange S.A., rebranded as Orange, formerly France Télécom S.A., stylized as france telecom, is a French multinational telecommunications corporation. It has 266 million customers worldwide and employs 89,000 people in France, and 59,000 elsewhere.    \n",
       "4                                                                                                                                                                Air France, stylised as AIRFRANCE, is the flag carrier of France headquartered in Tremblay-en-France. It is a subsidiary of the Air France–KLM Group and a founding member of the SkyTeam global airline alliance.    \n",
       "5                                                                                                                                                                                 Crédit Agricole Group, sometimes called La banque verte due to its historical ties to farming, is a French international banking group and the world's largest cooperative financial institution.    \n",
       "6                                                                                                           Maisons du Monde is a French furniture and home decor company founded in Brest in 1996 by Xavier Marie. At the end of 2015 it had nearly 250 stores across France, Italy, Spain, Luxembourg, Belgium, Germany and in Switzerland, of which more than 180 are in France.    \n",
       "7                                                                                                                                                                                                                                         Électricité de France S.A., commonly known as EDF, is a French multinational electric utility company, largely owned by the French state.    \n",
       "8                                                                                                                                                             Castorama is a French retailer of DIY and home improvement tools and supplies, headquartered in Templemars, France, and is part of the British group Kingfisher plc, which has 101 stores in France and 90 in Poland.    \n",
       "9                                                                                                                                                                                                                           Miss France 2022 was the 92nd edition of the Miss France pageant. The competition was held on 11 December 2021 at the Zénith de Caen in Caen, Normandy.    \n",
       "\n",
       "                                             result.@type  \n",
       "0                                     [Thing, SportsTeam]  \n",
       "1  [SportsOrganization, Organization, Corporation, Thing]  \n",
       "2                      [Organization, Thing, Corporation]  \n",
       "3                      [Organization, Thing, Corporation]  \n",
       "4             [Organization, Corporation, Thing, Airline]  \n",
       "5                      [Organization, Thing, Corporation]  \n",
       "6                      [Organization, Thing, Corporation]  \n",
       "7                      [Organization, Thing, Corporation]  \n",
       "8               [Organization, Place, Thing, Corporation]  \n",
       "9                       [TouristAttraction, Event, Thing]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_knowledge_graph_df(\"France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90e1b960-5ba9-4b2c-b8e7-f6813350b968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkee_keywords(input):\n",
    "    \"\"\"\n",
    "    Main pipeline function which takes input and generates list of keywords using \n",
    "    wikipedia scraping and NLP.\n",
    "  \n",
    "    Parameters\n",
    "    ---------\n",
    "    input (string): the input word which is the final Linkee answer\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    final_keywords (list): the list of possible keywords\n",
    "    \"\"\"\n",
    "    answer_list = tidy_input(input)\n",
    "  \n",
    "    #Keyword extraction\n",
    "    text, key_url_terms = find_text(input)\n",
    "    \n",
    "    #Potentially here combine 1-gram, 2-gram, 3-gram results\n",
    "    words_df = pd.DataFrame()\n",
    "    words = (keyword_extract(text, 2))[0] #+ (keyword_extract(text, 1))[0] + (keyword_extract(text, 3))[0]\n",
    "    scores = (keyword_extract(text, 2))[1] #+ (keyword_extract(text, 1))[1] + (keyword_extract(text, 3))[1]\n",
    "    words_df['words'] = words\n",
    "    words_df['scores'] = scores\n",
    "    words_df.sort_values(by=['scores'],ascending=False)\n",
    "    words_df = words_df[:100]\n",
    "  \n",
    "    # Adding wikipedia page names to the words found at the top\n",
    "    words = key_url_terms + words\n",
    "  \n",
    "    #Cleaning up returned keywords\n",
    "    words2 = answer_keyword_compare(words, answer_list)\n",
    "  \n",
    "    words2 = remove_non_noun_full_keywords(words2)\n",
    "  \n",
    "    final_keywords = select_keywords(words2)\n",
    "  \n",
    "    return(final_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8989c6f2-39dd-487f-a8e8-8ca7a891fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleanup function to clean up the fact at the end\n",
    "def cleanup(s):\n",
    "    \"\"\" Cleans up string by removing certain characters \"\"\"\n",
    "    strip_refs = re.compile(\"\\.?\\[\\d+\\]?\")\n",
    "    s = strip_refs.sub(\"\", s).strip()\n",
    "    \n",
    "    # Pretty ugly\n",
    "    if s[-1] == \".\":\n",
    "        s = s[0:-1]\n",
    "        \n",
    "    return s\n",
    "\n",
    "def fill_in_blank_q_generate(final_input, input, facts = 1):\n",
    "    #Might break if only 1 fact possible and not 2 - needs fixed\n",
    "    \"\"\"\n",
    "    Generate a fill in the blank style question using 1 or 2 \n",
    "    facts about the input.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    final_input (string): original input (final answer) for card\n",
    "    input (string): answer/keyword which we want to generate question for\n",
    "    facts (integer 1/2): allows to keep 1 or 2 facts for each answer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    question (\"string\"): the fill in the blank question\n",
    "    num_statements (integer): the number of statements found corresponding to the \n",
    "                            input\n",
    "    \"\"\"\n",
    "    final_input = tidy_input(final_input)\n",
    "\n",
    "    if (facts < 1) or (facts > 2):\n",
    "        return(\"Invalid entry. Please specify if you want 1 or 2 facts.\")\n",
    "    page_name = input\n",
    "\n",
    "    #Get Wikipedia page and text\n",
    "    #using knowledge graph method\n",
    "    text, key_url = find_text(page_name, keep_words= 100000, cleanup = False, multi_links = False)\n",
    "    print(page_name)\n",
    "    # text = (wikipedia.page(page_name,auto_suggest=False)).content \n",
    "    text = re.sub(r'==.*?==+', '', text)\n",
    "    #text = text.replace('\\n', '')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub(\"\\s\\s+\", \" \", text)\n",
    "\n",
    "    #Get category of input to better search for entity\n",
    "    category = classify_input(get_knowledge_graph_df(input))\n",
    "    #If person, just look for surname when trying to find facts\n",
    "    if category == \"Person\":\n",
    "        page_entity = input.split()[-1]\n",
    "    else:\n",
    "        page_entity = input\n",
    "    #Above is fine\n",
    "\n",
    "    doc = nlp(text)\n",
    "    #Set up empty array to hold facts\n",
    "    uniqueStatements = []\n",
    "    #cue: Verb lemma with which ``entity`` is associated (e.g. \"be\", \"have\", \"say\").\n",
    "    #Potentially use pronouns for the entity as well and combine results\n",
    "    for cue in [\"be\", \"have\", \"say\", \"do\", \"win\", \"write\", \"talk\", \"talk about\", \"born\", \"receive\", \"make\", \"continue\", \"find\"]:\n",
    "        statements = textacy.extract.semistructured_statements(doclike = doc, entity = page_entity, cue = cue)\n",
    "        for statement in statements:\n",
    "            entity, verb, fact = statement\n",
    "            factlist = [str(word) for word in fact]\n",
    "            fact = cleanup(str(fact))\n",
    "\n",
    "            #Removing statements where target word/phrase appears in fact\n",
    "            # if re.search(page_name, fact):\n",
    "            #   continue\n",
    "      \n",
    "            # if len(answer_keyword_compare(factlist, page_name)) != \\\n",
    "            #    len(factlist):\n",
    "            #    print(f'removed: {factlist} due to page_name {page_name} ')\n",
    "            #if fact has answer word in it ignore\n",
    "            #  continue\n",
    "            if len(answer_keyword_compare(factlist, final_input)) != len(factlist):\n",
    "                #if fact has final answer in it ignore \n",
    "                print(f'removed: {factlist} due to final_input {final_input} ')\n",
    "                continue\n",
    "            #Remove statements that are too long - more than 35 words long\n",
    "            if len(fact.split()) > 35:\n",
    "                continue\n",
    "            elif len(fact.split()) < 5:\n",
    "                continue\n",
    "            # statement = f\"{page_name} {verb} {fact}\"\n",
    "            statement = f\"{verb} {fact}\"\n",
    "            #More cleanup on fact\n",
    "            statement = re.sub(r\"[\\[\\]\\•\\▽\\❖\\†]+\", '', statement)\n",
    "            statement = statement.replace(', ', ' ')\n",
    "            statement = statement.replace(' , ', ', ')\n",
    "            statement = statement.replace(' ( ', ' (')\n",
    "            statement = statement.replace(' )', ')')\n",
    "            statement = statement.replace(\" 's\", \"'s\")\n",
    "            statement = statement.replace(\" - \", \"-\")\n",
    "            statement = statement.replace(\"\\'s\", \"'s\")\n",
    "            statement = statement.replace(page_name, '______')\n",
    "            statement = f\"{page_name} {statement}\"\n",
    "\n",
    "            uniqueStatements.append(statement)\n",
    "    num_statements = len(uniqueStatements)\n",
    "\n",
    "    #If it can't find any facts, should we try splitting up the input in\n",
    "    #a different manner - at moment, just telling us this is happening\n",
    "    if len(uniqueStatements) == 0:\n",
    "        return(\"No facts found for input.\", num_statements)\n",
    "\n",
    "    #Ensure code doesn't break if 2 facts are asked for but not available\n",
    "    if len(uniqueStatements) == 1 and facts == 2:\n",
    "        print('Only one fact available for answer.')\n",
    "        facts = 1\n",
    "\n",
    "    #Good tags for finding facts are numbers, proper nouns,\n",
    "    #foreign words and comparative/superlative adjectives/adverbs\n",
    "    good_tags = ['CD', 'FW', 'JJR', 'JJS', 'NNP', 'NNPS', 'RBR', 'RBRS']\n",
    "    tag_count = []\n",
    "    for i in range(len(uniqueStatements)):\n",
    "        tag_tuples = nltk.pos_tag(uniqueStatements[i].split())\n",
    "        tags = [x[1] for x in tag_tuples]\n",
    "        #Adding a small weight for statement length to prioritise longer facts \n",
    "        #which should have more info\n",
    "        tag_count.append(sum(x in good_tags for x in tags) + 0.3*len(tags))\n",
    "\n",
    "    #Returning a sorted DataFrame of all the questions to be able to view\n",
    "    df = pd.DataFrame(list(zip(uniqueStatements, tag_count)),\n",
    "                      columns =['Statement', 'Count'])\n",
    "    df = df.sort_values(by = 'Count', ascending = False)\n",
    "#     return(df)\n",
    "\n",
    "    #Get sorted array of indexes containing facts in ascending order of good tags\n",
    "    #In case of a tie, this arrays puts the lower numbered index first\n",
    "    sorted_count = sorted(range(len(tag_count)), key=lambda k: tag_count[k])\n",
    "\n",
    "    #Use 2 facts with most good tags in them\n",
    "\n",
    "    fact1 = uniqueStatements[sorted_count[-1]]\n",
    "    if facts == 2:\n",
    "        fact2 = uniqueStatements[sorted_count[-2]]\n",
    "\n",
    "    #Calculate how many letters are in the answer we are blanking\n",
    "    page_name_words = page_name\n",
    "    words = page_name_words.split()\n",
    "    letters_per_word = [len(w) for w in words]\n",
    "\n",
    "    fact1 = fact1.replace(page_name, '______ ' + str(letters_per_word))\n",
    "    if category == \"Person\" and facts == 2:\n",
    "        fact2 = fact2.replace(page_name, 'This person')\n",
    "    elif category != \"Person\" and facts == 2:\n",
    "        fact2 = fact2.replace(page_name, 'It')\n",
    "\n",
    "    if facts == 1:\n",
    "        question = str('Fill in the blank: ') + fact1 + str('.')\n",
    "    elif facts == 2:\n",
    "        question = str('Fill in the blank: ') + fact1 + str('. ') + fact2 + str('.')\n",
    "\n",
    "    return(question, num_statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7711466c-85a3-4cb0-abad-f372165e9f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_card(input):\n",
    "    '''Takes input and generates 4 question answer pairs'''\n",
    "    keywords = linkee_keywords(input)\n",
    "    answers = [] #Empty list to add answers we have questions for\n",
    "    questions = []\n",
    "    for answer in keywords:\n",
    "        try:\n",
    "            question, num_statements = fill_in_blank_q_generate(input, answer, facts = 2)\n",
    "        #except PageError: \n",
    "        except: \n",
    "            print(answer + ' does not have a Wikipedia page')\n",
    "            continue\n",
    "\n",
    "        if question == 'No facts found for input.':\n",
    "            print(answer + ' does not have facts')\n",
    "            continue\n",
    "        else:\n",
    "            print(answer + ' does have facts')\n",
    "            answers.append(answer)\n",
    "            questions.append(question)\n",
    "  \n",
    "    for i in range(len(answers)):\n",
    "        questions = [re.sub(answers[i], f\"[keyword {i+1}]\", qus) for qus in questions]\n",
    "    print(answers,\"; \",questions)\n",
    "    return(answers, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45aee657-eab2-4b4b-b322-48d568bb446e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academy Award\n",
      "Academy Award does not have facts\n",
      "Golden Globe\n",
      "Golden Globe does not have facts\n",
      "('Saving Private',)\n",
      "failed with No urls found for Saving Private\n",
      "Saving Private\n",
      "Saving Private does not have facts\n",
      "American actor\n",
      "American actor does not have facts\n",
      "Forrest Gump\n",
      "removed: ['six', 'Academy', 'Awards', ':', 'Best', 'Picture', ',', 'Best', 'Director', ',', 'Best', 'Actor', 'for', 'Hanks', ',', 'Best', 'Adapted', 'Screenplay', ',', 'Best', 'Visual', 'Effects', ',', 'and', 'Best', 'Film', 'Editing'] due to final_input ['TOM', 'HANK', 'tom', 'toms', 'TOMS', 'Tom', 'hanks', 'Hank', 'Hanks', 'Toms', 'hank', 'HANKS'] \n",
      "Forrest Gump does have facts\n",
      "('Lucky Guy', ['Lucky Guy (play)', 'Lucky Guy (musical)', '\"Lucky Guy\" (Dieter Bohlen song)', '\"Lucky Guy\" (Kim Hyun-joong song)', 'The Lucky Guy'])\n",
      "failed with No urls found for Lucky Guy\n",
      "Lucky Guy\n",
      "Only one fact available for answer.\n",
      "Lucky Guy does have facts\n",
      "North America\n",
      "North America does have facts\n",
      "Beautiful Day\n",
      "Beautiful Day does have facts\n",
      "Robert Langdon\n",
      "Robert Langdon does have facts\n",
      "Sheriff Woody\n",
      "Only one fact available for answer.\n",
      "Sheriff Woody does have facts\n",
      "Walt Disney\n",
      "Walt Disney does not have facts\n",
      "('Night Live',)\n",
      "failed with No urls found for Night Live\n",
      "Night Live\n",
      "Night Live does have facts\n",
      "('Life Achievement',)\n",
      "failed with No urls found for Life Achievement\n",
      "Life Achievement\n",
      "Life Achievement does not have facts\n",
      "('Cloud atlas (disambiguation)', ['International Cloud Atlas', 'Alexander George McAdie', 'Donald Platt', 'Cloud Atlas (novel)', 'Liam Callanan', 'Cloud Atlas (film)', 'Toshi Ichiyanagi', 'Mikel Rouse'])\n",
      "failed with No urls found for Cloud Atlas\n",
      "('could atlas',)\n",
      "failed with No urls found for Cloud Atlas\n",
      "Looking at wiki page for: Cloud_Atlas_(film)\n",
      "Cloud Atlas\n",
      "removed: ['a', '2012', 'epic', 'science', 'fiction', 'film', 'written', 'and', 'directed', 'by', 'the', 'Wachowskis', 'and', 'Tom', 'Tykwer'] due to final_input ['TOM', 'HANK', 'tom', 'toms', 'TOMS', 'Tom', 'hanks', 'Hank', 'Hanks', 'Toms', 'hank', 'HANKS'] \n",
      "Only one fact available for answer.\n",
      "Cloud Atlas does have facts\n",
      "('Brothers John',)\n",
      "failed with No urls found for Brothers John\n",
      "Brothers John\n",
      "Brothers John does not have facts\n",
      "Sony Pictures\n",
      "Sony Pictures does not have facts\n",
      "Fred Rogers\n",
      "Fred Rogers does not have facts\n",
      "Kennedy Center\n",
      "Kennedy Center does have facts\n",
      "Oakland California\n",
      "Oakland California does not have facts\n",
      "('Stone magazine',)\n",
      "failed with No urls found for Stone magazine\n",
      "Stone magazine\n",
      "Stone magazine does not have facts\n",
      "('Public Outreach',)\n",
      "failed with No urls found for Public Outreach\n",
      "Public Outreach\n",
      "Public Outreach does not have facts\n",
      "('Kip Wilson',)\n",
      "failed with No urls found for Kip Wilson\n",
      "Kip Wilson\n",
      "Kip Wilson does not have facts\n",
      "Greek Orthodox\n",
      "Greek Orthodox does not have facts\n",
      "('Philadelphias success',)\n",
      "failed with No urls found for Philadelphias success\n",
      "Philadelphias success\n",
      "Philadelphias success does not have facts\n",
      "('Oscars Spencer',)\n",
      "failed with No urls found for Oscars Spencer\n",
      "('oscar spencer',)\n",
      "failed with No urls found for Oscars Spencer\n",
      "Looking at wiki page for: Charlie_Chaplin\n",
      "Oscars Spencer\n",
      "Oscars Spencer does not have facts\n",
      "Joe Wright\n",
      "Joe Wright does have facts\n",
      "('Museum Capital',)\n",
      "failed with No urls found for Museum Capital\n",
      "Museum Capital\n",
      "Museum Capital does not have facts\n",
      "Premier League\n",
      "Premier League does have facts\n",
      "Apollo program\n",
      "Only one fact available for answer.\n",
      "Apollo program does have facts\n",
      "('Finch directed',)\n",
      "failed with No urls found for Finch directed\n",
      "('final director',)\n",
      "failed with No urls found for Finch directed\n",
      "Looking at wiki page for: All_the_Bright_Places_(film)\n",
      "Finch directed\n",
      "Finch directed does not have facts\n",
      "Tennessee Williams\n",
      "Tennessee Williams does have facts\n",
      "Larry Crowne\n",
      "removed: ['a', '2011', 'American', 'romantic', 'comedy', 'film', 'starring', 'Tom', 'Hanks', 'and', 'Julia', 'Roberts'] due to final_input ['TOM', 'HANK', 'tom', 'toms', 'TOMS', 'Tom', 'hanks', 'Hank', 'Hanks', 'Toms', 'hank', 'HANKS'] \n",
      "Larry Crowne does not have facts\n",
      "Prime Minister\n",
      "Only one fact available for answer.\n",
      "Prime Minister does have facts\n",
      "('Zemeckiss Cast',)\n",
      "failed with No urls found for Zemeckiss Cast\n",
      "('zemeckis case',)\n",
      "failed with No urls found for Zemeckiss Cast\n",
      "No valid keyword\n",
      "Zemeckiss Cast\n",
      "Zemeckiss Cast does not have a Wikipedia page\n",
      "Chabot College\n",
      "Chabot College does have facts\n",
      "('Uncommon Type',)\n",
      "failed with No urls found for Uncommon Type\n",
      "Uncommon Type\n",
      "Uncommon Type does not have facts\n",
      "('Ratliff announced',)\n",
      "failed with No urls found for Ratliff announced\n",
      "('ratcliffe announcer',)\n",
      "failed with No urls found for Ratliff announced\n",
      "Looking at wiki page for: Welcome_Home_(2018_film)\n",
      "Ratliff announced\n",
      "Ratliff announced does not have facts\n",
      "Broadway debut\n",
      "Broadway debut does not have facts\n",
      "War\n",
      "War does have facts\n",
      "('Rawley Farnsworth',)\n",
      "failed with No urls found for Rawley Farnsworth\n",
      "('ramsey farnsworth',)\n",
      "failed with No urls found for Rawley Farnsworth\n",
      "Looking at wiki page for: In_%26_Out_(film)\n",
      "Rawley Farnsworth\n",
      "Rawley Farnsworth does not have facts\n",
      "Kathleen Quinlan\n",
      "Only one fact available for answer.\n",
      "Kathleen Quinlan does have facts\n",
      "Frank Abagnale\n",
      "Frank Abagnale does have facts\n",
      "('Texas Congressman',)\n",
      "failed with No urls found for Texas Congressman\n",
      "Texas Congressman\n",
      "Texas Congressman does not have facts\n",
      "('Extremely Loud',)\n",
      "failed with No urls found for Extremely Loud\n",
      "Extremely Loud\n",
      "Extremely Loud does have facts\n",
      "('Luhrmann Shooting',)\n",
      "failed with No urls found for Luhrmann Shooting\n",
      "Luhrmann Shooting\n",
      "Luhrmann Shooting does not have a Wikipedia page\n",
      "('Chester Marlon',)\n",
      "failed with No urls found for Chester Marlon\n",
      "('chester marion',)\n",
      "failed with No urls found for Chester Marlon\n",
      "Looking at wiki page for: Chet_Hanks\n",
      "Chester Marlon\n",
      "Chester Marlon does not have facts\n",
      "('Schöneck Hesse',)\n",
      "failed with No urls found for Schöneck Hesse\n",
      "Schöneck Hesse\n",
      "Schöneck Hesse does not have facts\n",
      "('Takis Theodorikakos',)\n",
      "failed with No urls found for Takis Theodorikakos\n",
      "('takes theodorakakos',)\n",
      "failed with No urls found for Takis Theodorikakos\n",
      " No wiki urls: 1st pass\n",
      "Searching for urls with input Takis Theodorikakos\n",
      "no wiki pages found\n",
      "Takis Theodorikakos\n",
      "Takis Theodorikakos does not have facts\n",
      "('Governor Jerry',)\n",
      "failed with No urls found for Governor Jerry\n",
      "Governor Jerry\n",
      "Governor Jerry does not have facts\n",
      "('Portuguese descent',)\n",
      "failed with No urls found for Portuguese descent\n",
      "Portuguese descent\n",
      "Portuguese descent does not have facts\n",
      "Moon\n",
      "Moon does have facts\n",
      "July\n",
      "July does have facts\n",
      "('Band', ['Bánd', 'Band, Iran', 'Band, Mureș', 'Band-e Majid Khan', 'Band (surname)', 'Musical ensemble', 'Band (rock and pop)', 'Concert band', 'Dansband', 'Jazz band', 'Marching band', 'School band', 'The Band', 'The Band (album)', 'Mando Diao', 'Rede Bandeirantes', 'The Band (film)', 'The Band (musical)', 'Armband', 'Smart band', 'Microsoft Band', 'Bandolier', 'Bands (neckwear)', 'Belt (clothing)', 'Strap', 'Wedding band', 'Bands (Italian Army irregulars)', 'Female order of the Band', 'Order of the Band', 'Band (algebra)', 'Band (order theory)', 'Band (radio)', 'Frequency band', 'LTE frequency bands', 'Shortwave bands', 'UMTS frequency bands', 'BAND (software)', 'Band cell', 'Bird banding', 'Electronic band structure', 'Gastric band', 'Signaling (telecommunications)', 'In-band signaling', 'Out-of-band', 'origin of birds', 'Band (First Nations Canada)', 'Band society', 'Tribe (Native American)', 'Rubber band', 'The Band (professional wrestling)', 'All pages with titles beginning with Band', 'All pages with titles containing Band', 'Band of Brothers (disambiguation)', 'Bandage', 'Banding (disambiguation)', 'Bandy (disambiguation)', 'Bend (disambiguation)', 'Drum and bugle corps (disambiguation)', 'Herd', 'Ribbon (disambiguation)', 'Stripe (disambiguation)'])\n",
      "failed with No urls found for Band\n",
      "('and', ['Conjunction (grammar)', 'Logical conjunction', 'Bitwise AND', 'short-circuit operator', 'Ampersand', 'AND gate', 'And (John Martyn album)', 'And (Koda Kumi album)', 'A N D (Tricot album)', 'Jonah Matranga', 'Alberta New Democratic Party', 'Academy of Nutrition and Dietetics', 'Associated Northcliffe Digital', 'Automotive Navigation Data', 'AND Corporation', 'Fiverr', 'Anderson Regional Airport', 'Anderston railway station', 'Allow natural death', 'Andorra', 'Andromeda (constellation)', 'Ansus language', 'All pages with titles beginning with And', 'All pages with titles containing And', '& (disambiguation)', 'Ampersand (disambiguation)'])\n",
      "failed with No urls found for Band\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at wiki page for: Ring_(jewellery)\n",
      "Band\n",
      "Band does not have facts\n",
      "Spielberg\n",
      "Spielberg does have facts\n",
      "('Keatons alcoholic',)\n",
      "failed with No urls found for Keatons alcoholic\n",
      "('keating alcohol',)\n",
      "failed with No urls found for Keatons alcoholic\n",
      "No valid keyword\n",
      "Keatons alcoholic\n",
      "Keatons alcoholic does not have a Wikipedia page\n",
      "Toy\n",
      "Toy does not have facts\n",
      "Emmy\n",
      "Emmy does not have facts\n",
      "HBO\n",
      "HBO does have facts\n",
      "['Forrest Gump', 'Lucky Guy', 'North America', 'Beautiful Day', 'Robert Langdon', 'Sheriff Woody', 'Night Live', 'Cloud Atlas', 'Kennedy Center', 'Joe Wright', 'Premier League', 'Apollo program', 'Tennessee Williams', 'Prime Minister', 'Chabot College', 'War', 'Kathleen Quinlan', 'Frank Abagnale', 'Extremely Loud', 'Moon', 'July', 'Spielberg', 'HBO'] ;  ['Fill in the blank: ______ [7, 4] is a 1994 American comedy-drama film directed by Robert Zemeckis and written by Eric Roth. It won Best Picture, Best Actor.', 'Fill in the blank: ______ [5, 3] is a play by Nora Ephron that premiered in 2013, the year after her death.', 'Fill in the blank: ______ [5, 7] is the fourth most populous continent after Asia, Africa, and Europe. It is a very large continent that surpasses the Arctic Circle ,.', 'Fill in the blank: ______ [9, 3] was a different song called \" Always \", which was later released as a B-side. It is a song by Irish rock band U2.', 'Fill in the blank: ______ [6, 7] is a fictional character created by author Dan Brown for his ______ book series : Angels & Demons (2000). It has an adventure with the concepts of Freemasonry in Washington D. C..', 'Fill in the blank: ______ [7, 5] is a fictional, pull-string cowboy rag doll who appears in the Disney – Pixar Toy Story franchise.', 'Fill in the blank: ______ [5, 4] has received a number of awards, including 86 Primetime Emmy Awards, four Writers Guild of America Awards, and two Peabody Awards. It is an American late-night live television sketch comedy and variety show created by Lorne Michaels and developed by Dick Ebersol.', 'Fill in the blank: ______ [5, 5] is an immense sum total of not only the human experience.', 'Fill in the blank: ______ [7, 6] is the only U. S. institution that presents a free performance 365 days a year, daily at 6 pm (12 noon on December 24). It has three main theaters : the Concert Hall, the Opera House, and the Eisenhower Theater.', 'Fill in the blank: ______ [3, 6] is a British film director residing in Somerset, England. This person had an interest in the arts ,.', 'Fill in the blank: ______ [7, 6] has  representatives in the Association : Arsenal, Aston Villa, Chelsea, Everton, Fulham, Liverpool, Manchester City, Manchester United, Newcastle United and Tottenham Hotspur. It had title sponsorship rights sold to two companies, which were Carling brewery and Barclays Bank PLC.', 'Fill in the blank: ______ [6, 7] has been the focus of several works of fiction, including : Apollo 18, a 2011 horror movie which was released to negative reviews.', 'Fill in the blank: ______ [9, 8] was one of the inaugural honorees in the Rainbow Honor Walk, a walk of fame in San Francisco ’s Castro neighborhood noting LGBTQ people who have \" made significant contributions in their fields. This person had a lengthy relationship with Robert Carroll, a Vietnam veteran and aspiring writer in his 20s.', 'Fill in the blank: ______ [5, 8] can make a proposal which leaves enough room for amendments in order to keep the current discussion on the right tracks.', 'Fill in the blank: ______ [6, 7] is an accredited institutional member of the National Association of Schools of Art and Design. It was the first college opened by the Chabot-Las Positas Community College District.', 'Fill in the blank: ______ [3] is an intense armed conflict between states, governments, societies, or paramilitary groups such as mercenaries, insurgents, and militias. It had about the same number of casualties per capita as World ______ I.', 'Fill in the blank: ______ [8, 7] had an uncredited role in 1972s.', 'Fill in the blank: ______ [5, 8] was only 15 at the time. This person had been a featured speaker at an anti-crime seminar.', 'Fill in the blank: ______ [9, 4] is a 2011 American drama film directed by Stephen Daldry and written by Eric Roth. It had a limited release in the United States.', 'Fill in the blank: ______ [4] is a very slightly scalene ellipsoid due to tidal stretching, with its long axis displaced 30 ° from facing the Earth, due to gravitational anomalies from impact basins. It is the fifth largest natural satellite of the Solar System, categorizeable as one of its planetary-mass moons.', 'Fill in the blank: ______ [4] is the seventh month of the year (between June and August) in the Julian and Gregorian calendars and the fourth of seven months to have a length of 31 days. It is the Month of the Most Precious Blood of Jesus.', 'Fill in the blank: ______ [9] is the recipient of various accolades, including three Academy Awards (with two for Best Director), a [keyword 9] honor, a Cecil B. DeMille Award, and an AFI Life Achievement Award. It has honorary degrees from University of Southern California, 1994 Brown University, 1999 ; Yale University, 2002 ; Boston University, 2009 ; Harvard University, 2016.', 'Fill in the blank: ______ [3] would receive up to 75 New World films Showtime wo nt, which cost $ 50 million to sign a deal. It are the only American premium television services not to include live network feeds in their proprietary streaming VOD platforms.']\n"
     ]
    }
   ],
   "source": [
    "answers, questions = generate_card('Tom Hanks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62460f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Forrest Gump',\n",
       "  'Lucky Guy',\n",
       "  'North America',\n",
       "  'Beautiful Day',\n",
       "  'Robert Langdon',\n",
       "  'Sheriff Woody',\n",
       "  'Night Live',\n",
       "  'Cloud Atlas',\n",
       "  'Kennedy Center',\n",
       "  'Joe Wright',\n",
       "  'Premier League',\n",
       "  'Apollo program',\n",
       "  'Tennessee Williams',\n",
       "  'Prime Minister',\n",
       "  'Chabot College',\n",
       "  'War',\n",
       "  'Kathleen Quinlan',\n",
       "  'Frank Abagnale',\n",
       "  'Extremely Loud',\n",
       "  'Moon',\n",
       "  'July',\n",
       "  'Spielberg',\n",
       "  'HBO'],\n",
       " ['Fill in the blank: ______ [7, 4] is a 1994 American comedy-drama film directed by Robert Zemeckis and written by Eric Roth. It won Best Picture, Best Actor.',\n",
       "  'Fill in the blank: ______ [5, 3] is a play by Nora Ephron that premiered in 2013, the year after her death.',\n",
       "  'Fill in the blank: ______ [5, 7] is the fourth most populous continent after Asia, Africa, and Europe. It is a very large continent that surpasses the Arctic Circle ,.',\n",
       "  'Fill in the blank: ______ [9, 3] was a different song called \" Always \", which was later released as a B-side. It is a song by Irish rock band U2.',\n",
       "  'Fill in the blank: ______ [6, 7] is a fictional character created by author Dan Brown for his ______ book series : Angels & Demons (2000). It has an adventure with the concepts of Freemasonry in Washington D. C..',\n",
       "  'Fill in the blank: ______ [7, 5] is a fictional, pull-string cowboy rag doll who appears in the Disney – Pixar Toy Story franchise.',\n",
       "  'Fill in the blank: ______ [5, 4] has received a number of awards, including 86 Primetime Emmy Awards, four Writers Guild of America Awards, and two Peabody Awards. It is an American late-night live television sketch comedy and variety show created by Lorne Michaels and developed by Dick Ebersol.',\n",
       "  'Fill in the blank: ______ [5, 5] is an immense sum total of not only the human experience.',\n",
       "  'Fill in the blank: ______ [7, 6] is the only U. S. institution that presents a free performance 365 days a year, daily at 6 pm (12 noon on December 24). It has three main theaters : the Concert Hall, the Opera House, and the Eisenhower Theater.',\n",
       "  'Fill in the blank: ______ [3, 6] is a British film director residing in Somerset, England. This person had an interest in the arts ,.',\n",
       "  'Fill in the blank: ______ [7, 6] has  representatives in the Association : Arsenal, Aston Villa, Chelsea, Everton, Fulham, Liverpool, Manchester City, Manchester United, Newcastle United and Tottenham Hotspur. It had title sponsorship rights sold to two companies, which were Carling brewery and Barclays Bank PLC.',\n",
       "  'Fill in the blank: ______ [6, 7] has been the focus of several works of fiction, including : Apollo 18, a 2011 horror movie which was released to negative reviews.',\n",
       "  'Fill in the blank: ______ [9, 8] was one of the inaugural honorees in the Rainbow Honor Walk, a walk of fame in San Francisco ’s Castro neighborhood noting LGBTQ people who have \" made significant contributions in their fields. This person had a lengthy relationship with Robert Carroll, a Vietnam veteran and aspiring writer in his 20s.',\n",
       "  'Fill in the blank: ______ [5, 8] can make a proposal which leaves enough room for amendments in order to keep the current discussion on the right tracks.',\n",
       "  'Fill in the blank: ______ [6, 7] is an accredited institutional member of the National Association of Schools of Art and Design. It was the first college opened by the Chabot-Las Positas Community College District.',\n",
       "  'Fill in the blank: ______ [3] is an intense armed conflict between states, governments, societies, or paramilitary groups such as mercenaries, insurgents, and militias. It had about the same number of casualties per capita as World ______ I.',\n",
       "  'Fill in the blank: ______ [8, 7] had an uncredited role in 1972s.',\n",
       "  'Fill in the blank: ______ [5, 8] was only 15 at the time. This person had been a featured speaker at an anti-crime seminar.',\n",
       "  'Fill in the blank: ______ [9, 4] is a 2011 American drama film directed by Stephen Daldry and written by Eric Roth. It had a limited release in the United States.',\n",
       "  'Fill in the blank: ______ [4] is a very slightly scalene ellipsoid due to tidal stretching, with its long axis displaced 30 ° from facing the Earth, due to gravitational anomalies from impact basins. It is the fifth largest natural satellite of the Solar System, categorizeable as one of its planetary-mass moons.',\n",
       "  'Fill in the blank: ______ [4] is the seventh month of the year (between June and August) in the Julian and Gregorian calendars and the fourth of seven months to have a length of 31 days. It is the Month of the Most Precious Blood of Jesus.',\n",
       "  'Fill in the blank: ______ [9] is the recipient of various accolades, including three Academy Awards (with two for Best Director), a [keyword 9] honor, a Cecil B. DeMille Award, and an AFI Life Achievement Award. It has honorary degrees from University of Southern California, 1994 Brown University, 1999 ; Yale University, 2002 ; Boston University, 2009 ; Harvard University, 2016.',\n",
       "  'Fill in the blank: ______ [3] would receive up to 75 New World films Showtime wo nt, which cost $ 50 million to sign a deal. It are the only American premium television services not to include live network feeds in their proprietary streaming VOD platforms.'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers, questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c0bd98e-19c4-464c-a123-d048889aadf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['World War',\n",
       " 'Kansas City',\n",
       " 'Gram Studio',\n",
       " 'York Times',\n",
       " 'American Broadcasting',\n",
       " 'Los Angeles',\n",
       " 'Achievement Awards',\n",
       " 'Silly Symphony',\n",
       " 'Mary Poppins',\n",
       " 'Animated Short',\n",
       " 'Chicago Academy',\n",
       " 'Tomorrow EPCOT',\n",
       " 'Experimental Prototype',\n",
       " 'Company ABC',\n",
       " 'Winter Olympics',\n",
       " 'Golden Globe',\n",
       " 'Jungle Book',\n",
       " 'Happiest Millionaire',\n",
       " 'Park School',\n",
       " 'Pinocchio Fantasia',\n",
       " 'Museum records',\n",
       " 'Iwerks',\n",
       " 'Bambi Dumbo',\n",
       " 'Roy started',\n",
       " 'Neal Gabler',\n",
       " 'South America',\n",
       " 'year theyre',\n",
       " 'Thailands Order',\n",
       " 'Pato Donald',\n",
       " 'Construction work',\n",
       " 'Virginia Davis',\n",
       " 'Red Cross',\n",
       " 'Mickey',\n",
       " 'Rudolf Ising',\n",
       " 'Carolwood Pacific',\n",
       " 'Der König',\n",
       " 'Playwright Robert',\n",
       " 'Floyd Norman',\n",
       " 'Arts colloquially',\n",
       " 'Tripp Avenue',\n",
       " 'Aztec Eagle',\n",
       " 'Alfonso Cuaron',\n",
       " 'Kenneth Branagh',\n",
       " 'Lucky Rabbit',\n",
       " 'Annette Funicello',\n",
       " 'Stephan Jungk',\n",
       " 'Movie Database',\n",
       " 'White',\n",
       " 'Kimball argues',\n",
       " 'Laugh',\n",
       " 'July',\n",
       " 'Elias']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkee_keywords('Walt Disney')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bba0cf21-af77-4506-93f3-820764406d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': {'code': 400,\n",
       "  'message': 'API key not valid. Please pass a valid API key.',\n",
       "  'status': 'INVALID_ARGUMENT',\n",
       "  'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo',\n",
       "    'reason': 'API_KEY_INVALID',\n",
       "    'domain': 'googleapis.com',\n",
       "    'metadata': {'service': 'kgsearch.googleapis.com'}}]}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_knowledge_graph('Tom Hank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ae96982-6cf9-4062-8040-6080c496be28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past here Disney\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statement</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Disney continued to produce cartoons with Mickey Mouse and other characters</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Disney continued to focus its talents on television throughout the 1950s</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     Statement  \\\n",
       "0  Disney continued to produce cartoons with Mickey Mouse and other characters   \n",
       "1     Disney continued to focus its talents on television throughout the 1950s   \n",
       "\n",
       "   Count  \n",
       "0    6.3  \n",
       "1    4.3  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_in_blank_q_generate('Tom Hanks', 'Disney', facts = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7ae70f-3bc3-44bf-a199-f97bddb59468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "677efd62-03dc-4efe-8c77-eb5e39562e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bb65a1f-5377-473e-9754-9ffc2b9688f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Statement'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2938838f-c462-4328-a199-31867bca8e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resultScore</th>\n",
       "      <th>@type</th>\n",
       "      <th>result.url</th>\n",
       "      <th>result.detailedDescription.license</th>\n",
       "      <th>result.detailedDescription.url</th>\n",
       "      <th>result.detailedDescription.articleBody</th>\n",
       "      <th>result.image.contentUrl</th>\n",
       "      <th>result.image.url</th>\n",
       "      <th>result.@type</th>\n",
       "      <th>result.@id</th>\n",
       "      <th>result.name</th>\n",
       "      <th>result.description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31381.007812</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>http://disney.com</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Walt_Disney_...</td>\n",
       "      <td>The Walt Disney Company, commonly known as Dis...</td>\n",
       "      <td>https://encrypted-tbn2.gstatic.com/images?q=tb...</td>\n",
       "      <td>https://ar.m.wikipedia.org/wiki/%D9%85%D9%84%D...</td>\n",
       "      <td>[Corporation, Organization, Thing, TheaterGroup]</td>\n",
       "      <td>kg:/m/09b3v</td>\n",
       "      <td>The Walt Disney Company</td>\n",
       "      <td>Entertainment company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15199.707031</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>http://www.waltdisney.com</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Walt_Disney</td>\n",
       "      <td>Walter Elias Disney was an American entreprene...</td>\n",
       "      <td>https://encrypted-tbn1.gstatic.com/images?q=tb...</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:Walt_D...</td>\n",
       "      <td>[Thing, Person]</td>\n",
       "      <td>kg:/m/081nh</td>\n",
       "      <td>Walt Disney</td>\n",
       "      <td>American entrepreneur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9252.871094</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>http://movies.disney.com/the-lion-king</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Lion_King</td>\n",
       "      <td>The Lion King is a 1994 American animated musi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CreativeWork, Movie, Thing]</td>\n",
       "      <td>kg:/m/0m63c</td>\n",
       "      <td>The Lion King</td>\n",
       "      <td>1994 film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6824.843262</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>http://movies.disney.com/the-little-mermaid</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Little_Merma...</td>\n",
       "      <td>The Little Mermaid is a 1989 American animated...</td>\n",
       "      <td>https://encrypted-tbn2.gstatic.com/images?q=tb...</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:The_Li...</td>\n",
       "      <td>[CreativeWork, Movie, Thing]</td>\n",
       "      <td>kg:/m/01ry_x</td>\n",
       "      <td>The Little Mermaid</td>\n",
       "      <td>1989 film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6777.649902</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Hayao_Miyazaki</td>\n",
       "      <td>Hayao Miyazaki is a Japanese animator, directo...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:Hayao_...</td>\n",
       "      <td>[Thing, Person]</td>\n",
       "      <td>kg:/m/0534v</td>\n",
       "      <td>Hayao Miyazaki</td>\n",
       "      <td>Japanese animation director</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5211.307617</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Lady_and_the_Tramp</td>\n",
       "      <td>Lady and the Tramp is a 1955 American animated...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CreativeWork, Movie, Thing]</td>\n",
       "      <td>kg:/m/01q3w7</td>\n",
       "      <td>Lady and the Tramp</td>\n",
       "      <td>1955 film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5121.747070</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>http://movies.disney.com/cinderella</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cinderella_(1950...</td>\n",
       "      <td>Cinderella is a 1950 American animated musical...</td>\n",
       "      <td>https://encrypted-tbn2.gstatic.com/images?q=tb...</td>\n",
       "      <td>https://es.wikipedia.org/wiki/Archivo:1950_is_...</td>\n",
       "      <td>[CreativeWork, Movie, Thing]</td>\n",
       "      <td>kg:/m/023p33</td>\n",
       "      <td>Cinderella</td>\n",
       "      <td>1950 film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3643.347412</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>http://movies.disney.com/aladdin</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Aladdin_(1992_Di...</td>\n",
       "      <td>Aladdin is a 1992 American animated musical fa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CreativeWork, Movie, Thing]</td>\n",
       "      <td>kg:/m/0jnwx</td>\n",
       "      <td>Aladdin</td>\n",
       "      <td>1992 film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3165.555420</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>http://www.disneyanimation.com/</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Walt_Disney_Anim...</td>\n",
       "      <td>Walt Disney Animation Studios, sometimes short...</td>\n",
       "      <td>https://encrypted-tbn1.gstatic.com/images?q=tb...</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:Walt_D...</td>\n",
       "      <td>[Organization, Thing, Corporation]</td>\n",
       "      <td>kg:/m/04rcl7</td>\n",
       "      <td>Walt Disney Animation Studios</td>\n",
       "      <td>Animation company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3002.821777</td>\n",
       "      <td>EntitySearchResult</td>\n",
       "      <td>http://movies.disney.com/</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Text_o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Walt_Disney_Pict...</td>\n",
       "      <td>Walt Disney Pictures is an American film produ...</td>\n",
       "      <td>https://encrypted-tbn3.gstatic.com/images?q=tb...</td>\n",
       "      <td>https://ar.m.wikipedia.org/wiki/%D9%85%D9%84%D...</td>\n",
       "      <td>[Organization, Thing, Corporation]</td>\n",
       "      <td>kg:/m/01795t</td>\n",
       "      <td>Walt Disney Pictures</td>\n",
       "      <td>Film studio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    resultScore               @type  \\\n",
       "0  31381.007812  EntitySearchResult   \n",
       "1  15199.707031  EntitySearchResult   \n",
       "2   9252.871094  EntitySearchResult   \n",
       "3   6824.843262  EntitySearchResult   \n",
       "4   6777.649902  EntitySearchResult   \n",
       "5   5211.307617  EntitySearchResult   \n",
       "6   5121.747070  EntitySearchResult   \n",
       "7   3643.347412  EntitySearchResult   \n",
       "8   3165.555420  EntitySearchResult   \n",
       "9   3002.821777  EntitySearchResult   \n",
       "\n",
       "                                    result.url  \\\n",
       "0                            http://disney.com   \n",
       "1                    http://www.waltdisney.com   \n",
       "2       http://movies.disney.com/the-lion-king   \n",
       "3  http://movies.disney.com/the-little-mermaid   \n",
       "4                                          NaN   \n",
       "5                                          NaN   \n",
       "6          http://movies.disney.com/cinderella   \n",
       "7             http://movies.disney.com/aladdin   \n",
       "8              http://www.disneyanimation.com/   \n",
       "9                    http://movies.disney.com/   \n",
       "\n",
       "                  result.detailedDescription.license  \\\n",
       "0  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "1  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "2  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "3  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "4  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "5  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "6  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "7  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "8  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "9  https://en.wikipedia.org/wiki/Wikipedia:Text_o...   \n",
       "\n",
       "                      result.detailedDescription.url  \\\n",
       "0  https://en.wikipedia.org/wiki/The_Walt_Disney_...   \n",
       "1          https://en.wikipedia.org/wiki/Walt_Disney   \n",
       "2        https://en.wikipedia.org/wiki/The_Lion_King   \n",
       "3  https://en.wikipedia.org/wiki/The_Little_Merma...   \n",
       "4       https://en.wikipedia.org/wiki/Hayao_Miyazaki   \n",
       "5   https://en.wikipedia.org/wiki/Lady_and_the_Tramp   \n",
       "6  https://en.wikipedia.org/wiki/Cinderella_(1950...   \n",
       "7  https://en.wikipedia.org/wiki/Aladdin_(1992_Di...   \n",
       "8  https://en.wikipedia.org/wiki/Walt_Disney_Anim...   \n",
       "9  https://en.wikipedia.org/wiki/Walt_Disney_Pict...   \n",
       "\n",
       "              result.detailedDescription.articleBody  \\\n",
       "0  The Walt Disney Company, commonly known as Dis...   \n",
       "1  Walter Elias Disney was an American entreprene...   \n",
       "2  The Lion King is a 1994 American animated musi...   \n",
       "3  The Little Mermaid is a 1989 American animated...   \n",
       "4  Hayao Miyazaki is a Japanese animator, directo...   \n",
       "5  Lady and the Tramp is a 1955 American animated...   \n",
       "6  Cinderella is a 1950 American animated musical...   \n",
       "7  Aladdin is a 1992 American animated musical fa...   \n",
       "8  Walt Disney Animation Studios, sometimes short...   \n",
       "9  Walt Disney Pictures is an American film produ...   \n",
       "\n",
       "                             result.image.contentUrl  \\\n",
       "0  https://encrypted-tbn2.gstatic.com/images?q=tb...   \n",
       "1  https://encrypted-tbn1.gstatic.com/images?q=tb...   \n",
       "2                                                NaN   \n",
       "3  https://encrypted-tbn2.gstatic.com/images?q=tb...   \n",
       "4  https://encrypted-tbn0.gstatic.com/images?q=tb...   \n",
       "5                                                NaN   \n",
       "6  https://encrypted-tbn2.gstatic.com/images?q=tb...   \n",
       "7                                                NaN   \n",
       "8  https://encrypted-tbn1.gstatic.com/images?q=tb...   \n",
       "9  https://encrypted-tbn3.gstatic.com/images?q=tb...   \n",
       "\n",
       "                                    result.image.url  \\\n",
       "0  https://ar.m.wikipedia.org/wiki/%D9%85%D9%84%D...   \n",
       "1  https://commons.wikimedia.org/wiki/File:Walt_D...   \n",
       "2                                                NaN   \n",
       "3  https://commons.wikimedia.org/wiki/File:The_Li...   \n",
       "4  https://commons.wikimedia.org/wiki/File:Hayao_...   \n",
       "5                                                NaN   \n",
       "6  https://es.wikipedia.org/wiki/Archivo:1950_is_...   \n",
       "7                                                NaN   \n",
       "8  https://commons.wikimedia.org/wiki/File:Walt_D...   \n",
       "9  https://ar.m.wikipedia.org/wiki/%D9%85%D9%84%D...   \n",
       "\n",
       "                                       result.@type    result.@id  \\\n",
       "0  [Corporation, Organization, Thing, TheaterGroup]   kg:/m/09b3v   \n",
       "1                                   [Thing, Person]   kg:/m/081nh   \n",
       "2                      [CreativeWork, Movie, Thing]   kg:/m/0m63c   \n",
       "3                      [CreativeWork, Movie, Thing]  kg:/m/01ry_x   \n",
       "4                                   [Thing, Person]   kg:/m/0534v   \n",
       "5                      [CreativeWork, Movie, Thing]  kg:/m/01q3w7   \n",
       "6                      [CreativeWork, Movie, Thing]  kg:/m/023p33   \n",
       "7                      [CreativeWork, Movie, Thing]   kg:/m/0jnwx   \n",
       "8                [Organization, Thing, Corporation]  kg:/m/04rcl7   \n",
       "9                [Organization, Thing, Corporation]  kg:/m/01795t   \n",
       "\n",
       "                     result.name           result.description  \n",
       "0        The Walt Disney Company        Entertainment company  \n",
       "1                    Walt Disney        American entrepreneur  \n",
       "2                  The Lion King                    1994 film  \n",
       "3             The Little Mermaid                    1989 film  \n",
       "4                 Hayao Miyazaki  Japanese animation director  \n",
       "5             Lady and the Tramp                    1955 film  \n",
       "6                     Cinderella                    1950 film  \n",
       "7                        Aladdin                    1992 film  \n",
       "8  Walt Disney Animation Studios            Animation company  \n",
       "9           Walt Disney Pictures                  Film studio  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_knowledge_graph_df('Walt Disney')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cccb2a-6983-410c-b743-c1913984c6f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
