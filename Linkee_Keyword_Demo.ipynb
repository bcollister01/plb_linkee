{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linkee Keyword Demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bcollister01/plb_linkee/blob/main/Linkee_Keyword_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEKutbRpI-E_"
      },
      "source": [
        "Input phrases we are testing: Tom Hanks, Las Vegas, Olympics, Harry Potter, Cereal, Kitchen, Busted, Trees, Insurance companies, Hurricanes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTBrgNHbFfON"
      },
      "source": [
        "%%capture\n",
        "!pip install wikipedia\n",
        "!pip install yake\n",
        "!pip install --upgrade ecommercetools\n",
        "!pip install pattern\n",
        "!pip install textacy\n",
        "\n",
        "# Import packages\n",
        "import wikipedia\n",
        "import re\n",
        "import yake\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ecommercetools import seo\n",
        "from pattern.text.en import singularize, pluralize\n",
        "#For Google Knowledge Graph API\n",
        "import requests\n",
        "import urllib\n",
        "import json\n",
        "from requests_html import HTML\n",
        "from requests_html import HTMLSession\n",
        "#if scraping paragraphs from first few webpages\n",
        "from bs4 import BeautifulSoup\n",
        "#For question generation\n",
        "import spacy\n",
        "import textacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geWLFTOouhP7"
      },
      "source": [
        "#**Creating mini functions to split up tasks**\n",
        "\n",
        "A lot of our functions help to clean up our results or are helper functions. The main ones are find_text and keyword_extract."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions for finding all versions of words\n",
        "\n",
        "We want to make sure that we don't have two very similar keywords e.g. sweet and sweets. So we'll need to check keywords against singular/plural forms."
      ],
      "metadata": {
        "id": "55C8VMRA_fji"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJUpOnd30pmA"
      },
      "source": [
        "def ending_pluralize(noun):\n",
        "  '''Return most appropriate plural of input word.'''\n",
        "  if re.search('[sxz]$', noun):\n",
        "      return re.sub('$', 'es', noun)\n",
        "  elif re.search('[^aeioudgkprt]h$', noun):\n",
        "      return re.sub('$', 'es', noun)\n",
        "  elif re.search('y$', noun):\n",
        "      return re.sub('y$', 'ies', noun)\n",
        "  else:\n",
        "      return noun\n",
        "\n",
        "def add_s_pluralize(noun):\n",
        "  '''Naively add s to end of input word to create plural'''\n",
        "  return noun + 's'\n",
        "\n",
        "def tidy_input(input):\n",
        "  '''Take input word and tidy it up to create a list of options.\n",
        "  \n",
        "  We have a few different pluralize functions just to account for any\n",
        "  misspellings online/words created when punctuation removed.\n",
        "  '''\n",
        "\n",
        "  input_words = input.split()\n",
        "\n",
        "  #Add singular forms of plurals and plural forms of singles \n",
        "  singles = [singularize(plural) for plural in input_words]\n",
        "  plurals1 = [pluralize(single) for single in singles]\n",
        "  plurals2 = [ending_pluralize(single) for single in singles]\n",
        "  plurals3 = [add_s_pluralize(single) for single in singles]\n",
        "  input_words = input_words + singles + plurals1 + plurals2 + plurals3\n",
        "\n",
        "  input_words = input_words + [word.lower() for word in input_words]\n",
        "  #If you want capitalized words as well\n",
        "  input_words = input_words + [word[0].upper() + word[1:] for word in input.split()]\n",
        "  input_words = input_words + [word.upper() for word in input_words]\n",
        "\n",
        "  input_words = list(set(input_words))\n",
        "  \n",
        "  return(input_words)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions for working with Google Knowledge Graph\n",
        "\n",
        "We explored a few different methods for getting text to run through the keyword generation model. The Google Knowledge Graph was the most reliable of these but work is needed to interact with the API. We can also choose to adapt our search term slightly if we know from the Knowledge Graph what sort of enetity we are dealing with to potentially get better results."
      ],
      "metadata": {
        "id": "lsRpxhva_5fA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Next few functions sourced from https://practicaldatascience.co.uk/data-science/how-to-access-the-google-knowledge-graph-search-api\n",
        "\n",
        "def get_source(url):\n",
        "    \"\"\"Return the source code for the provided URL. \n",
        "\n",
        "    Args: \n",
        "        url (string): URL of the page to scrape.\n",
        "\n",
        "    Returns:\n",
        "        response (object): HTTP response object from requests_html. \n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        session = HTMLSession()\n",
        "        response = session.get(url)\n",
        "        return response\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(e)\n",
        "\n",
        "def get_knowledge_graph(api_key, query):\n",
        "    \"\"\"Return a Google Knowledge Graph for a given query.\n",
        "\n",
        "    Args: \n",
        "        api_key (string): Google Knowledge Graph API key. \n",
        "        query (string): Term to search for.\n",
        "\n",
        "    Returns:\n",
        "        response (object): Knowledge Graph response object in JSON format.\n",
        "    \"\"\" \n",
        "        \n",
        "    endpoint = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
        "    params = {\n",
        "        'query': query,\n",
        "        'limit': 10,\n",
        "        'indent': True,\n",
        "        'key': api_key,\n",
        "    }\n",
        "\n",
        "    url = endpoint + '?' + urllib.parse.urlencode(params)    \n",
        "    response = get_source(url)\n",
        "    \n",
        "    return json.loads(response.text)\n",
        "\n",
        "def get_knowledge_graph_df(input):\n",
        "  \"\"\"\n",
        "  Uses Google's knowledge graph to generate Pandas DataFrame of entities \n",
        "  deemed most similar to input searched. DataFrame includes categorization\n",
        "  of entity, title, short description and URL (usually to Wikipedia).\n",
        "  You will need to have set up an API key in Google Cloud Console to get this\n",
        "  to work (it's free to do and you can do 100k requests a day I believe.)\n",
        "  https://console.cloud.google.com/apis \n",
        "  Args:\n",
        "    input (string): Final Linkee answer\n",
        "\n",
        "  Returns:\n",
        "    knowledge_graph_df (Pandas DataFrame): info on Knowledge Graph results\n",
        "  \"\"\"\n",
        "  threshold=0.2\n",
        "  api_key = ####Removed for demo\n",
        "  knowledge_graph_json = get_knowledge_graph(api_key, input)\n",
        "  knowledge_graph_df = pd.json_normalize(knowledge_graph_json, record_path='itemListElement')\n",
        "  #Only using scores if knowledge graph actually returns something\n",
        "  if len(knowledge_graph_df) > 0:\n",
        "    max_score = max(knowledge_graph_df['resultScore'])\n",
        "    knowledge_graph_df = knowledge_graph_df.loc[knowledge_graph_df['resultScore']>threshold*max_score]\n",
        "  return knowledge_graph_df\n",
        "\n",
        "def classify_input(knowledge_graph_df):\n",
        "  \"\"\"Classify the input word/phrase as a certain category \n",
        "  to improve search results. Acts as failsafe if initial search\n",
        "  of input fails.\n",
        "\n",
        "  Args:\n",
        "    knowledge_graph_df: Return of get_knowledge_graph_df\n",
        "\n",
        "  Returns:\n",
        "    category (string): Category of input\n",
        "  \n",
        "  \"\"\"\n",
        "  if \"SportsTeam\" in knowledge_graph_df['result.@type'][0]:\n",
        "    entity_tags = knowledge_graph_df['result.@type'][1]\n",
        "  else:\n",
        "    entity_tags = knowledge_graph_df['result.@type'][0]\n",
        "  #return(entity_tags)\n",
        "  if (\"Movie\" in entity_tags) or (\"MovieSeries\" in entity_tags):\n",
        "    category = \"Movie\"\n",
        "  elif (\"TVEpisode\" in entity_tags) or (\"TVSeries\" in entity_tags):\n",
        "    category = \"TV\"\n",
        "  elif (\"VideoGame\" in entity_tags) or (\"VideoGameSeries\" in entity_tags):\n",
        "    category = \"VideoGame\"\n",
        "  elif (\"Book\" in entity_tags) or (\"BookSeries\" in entity_tags):\n",
        "    category = \"Book\"\n",
        "  elif \"Person\" in entity_tags:\n",
        "    category = \"Person\"\n",
        "  elif (\"MusicAlbum\" in entity_tags) or (\"MusicGroup\" in entity_tags) or (\"MusicRecording\" in entity_tags):\n",
        "    category = \"Music\"\n",
        "  elif (\"Place\" in entity_tags) or (\"AdministrativeArea\" in entity_tags):\n",
        "    category = \"Place\" \n",
        "  else:\n",
        "    category = \"Thing\"\n",
        "\n",
        "  return(category)\n",
        "\n",
        "def tailored_search(category, input):\n",
        "  \"\"\"Change the search to get better keywords for input, based on its category\n",
        "\n",
        "  Args:\n",
        "    category (string): Category of input\n",
        "    input (string): Final Linkee answer\n",
        "\n",
        "  Returns:\n",
        "    search_input (string): Search term to use to find keywords\n",
        "  \n",
        "  \"\"\"\n",
        "  if category == \"Movie\" or category == \"TV\" or category == \"Book\":\n",
        "    search_input = input + \" \" + category + \" information\"\n",
        "  elif category == \"Place\":\n",
        "    search_input = input + \" location\"\n",
        "  else:\n",
        "    search_input = input\n",
        "  return(search_input)\n",
        "\n",
        "\n",
        "def collect_urls(knowledge_graph_df):\n",
        "  \"\"\"Collect the urls from the knowledge graph to give more options to scrape \n",
        "  from.\n",
        "\n",
        "  Args:\n",
        "    knowledge_graph_df: Return of get_knowledge_graph_df\n",
        "\n",
        "  Returns:\n",
        "    list of urls (string): urls found\n",
        "  \n",
        "  \"\"\"\n",
        "  if 'result.detailedDescription.url' in knowledge_graph_df.columns:\n",
        "    knowledge_graph_df = knowledge_graph_df[knowledge_graph_df['result.detailedDescription.url'].notna()]\n",
        "    urlList = knowledge_graph_df['result.detailedDescription.url'].tolist()\n",
        "  else:\n",
        "    urlList = []\n",
        "  return urlList\n",
        "\n",
        "def get_wiki_links(urlList):\n",
        "  '''Extract the URLs linking to Wikipedia from a list of URLs'''\n",
        "  url_wiki=[urlList[i] for i in range(len(urlList)) if urlList[i].find(\"wiki\")!= -1]\n",
        "  # if len(url_wiki) == 0:\n",
        "  #   print('No Urls')\n",
        "  # if url_wiki:\n",
        "  return(url_wiki)\n",
        "\n",
        "\n",
        "def get_wiki_text(url_wiki, keep_words=10000):\n",
        "    '''\n",
        "  Takes a list of urls and scrapes from Wikipedia links\n",
        "  if present.\n",
        "  Input:\n",
        "    urlList - a list of urls\n",
        "    keep_words - the number of words to keep (approx up to paragraph)\n",
        "  Output:\n",
        "    text_comb - the text extracted from the paragraphs until word limit reached\n",
        "  '''\n",
        "    text_comb = ''\n",
        "    total_words = 0\n",
        "    # print('url_wiki',url_wiki)\n",
        "    for url in url_wiki:\n",
        "      wiki_term = url.split('/wiki/')[1]\n",
        "      print(f\"Looking at wiki page for: {wiki_term}\")\n",
        "      try:\n",
        "        text_wiki = (wikipedia.page(wiki_term, auto_suggest = False)).content\n",
        "      except KeyError: #fullurl errors can be caused by unicode or other symbols\n",
        "        text_wiki = (wikipedia.page(wiki_term, auto_suggest = True)).content\n",
        "      #This will drop headers surrounded by ==\n",
        "      text_wiki = re.sub(r'==.*?==+', '', text_wiki)\n",
        "      paras = text_wiki.split('\\n\\n')\n",
        "      word_count = len(paras[0].split()) #number of words in 1st paragraph\n",
        "      remaining_words = keep_words - total_words\n",
        "      j = 0\n",
        "      text = paras[0]\n",
        "      while word_count < remaining_words and j<len(paras)-1:\n",
        "        j += 1\n",
        "        para_text = paras[j]\n",
        "        word_count = word_count + len(para_text.split())\n",
        "        text = text + ' ' + para_text\n",
        "      #Drop new line /n clutter\n",
        "      text = text.replace('\\n', '')\n",
        "      text_comb = text_comb + text # change if want more than one\n",
        "      total_words = total_words + word_count\n",
        "      if total_words >= keep_words:\n",
        "        break  # break out of for loop when we have enough words\n",
        "    return text_comb\n",
        "\n",
        "\n",
        "def wiki_autosuggest(input, keep_words = 10000):\n",
        "  ''' Gets text from Wikipedia using whichever page is autosuggested\n",
        "      Input: input - original input word\n",
        "             keep_words - number of words to keep\n",
        "      Output: \n",
        "             text that has been cleaned\n",
        "  '''\n",
        "  # Get text from single wikipedia page using auto-suggest\n",
        "  try:\n",
        "    text_wiki = (wikipedia.page(input, auto_suggest = True)).content\n",
        "  except Exception as err:\n",
        "    print(err.args)\n",
        "    raise ValueError(f'No urls found for {input}') \n",
        "  #if no exception raised clean up text\n",
        "  #This will drop headers surrounded by ==\n",
        "  text_wiki = re.sub(r'==.*?==+', '', text_wiki)\n",
        "  paras = text_wiki.split('\\n\\n')\n",
        "  word_count=len(paras[0].split()) #number of words in 1st paragraph\n",
        "  j=0\n",
        "  text = paras[0]\n",
        "  while word_count < keep_words and j<len(paras)-1:\n",
        "    j += 1\n",
        "    para_text = paras[j]\n",
        "    word_count = word_count + len(para_text.split())\n",
        "    text = text + ' ' + para_text\n",
        "  #Drop new line /n clutter\n",
        "  text = text.replace('\\n', '')\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "ac8aBRhX_2QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main functions\n",
        "\n",
        "These functions find the text on the subject we want and then tries to find similar keywords to it."
      ],
      "metadata": {
        "id": "1JjwPARJAYHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def find_text(input): \n",
        "  '''  \n",
        "  Finds text related to input that can be used for\n",
        "  keyword extraction. This function attempts to clean\n",
        "  up the relevant text.\n",
        "  '''\n",
        "  knowledge_graph_df = get_knowledge_graph_df(input)\n",
        "  if len(knowledge_graph_df) == 0:\n",
        "    print(\"nothing found using knowledge graph, trying wiki\")\n",
        "    text = wiki_autosuggest(input)\n",
        "  else:\n",
        "    urlList = collect_urls(knowledge_graph_df)\n",
        "    url_wiki = get_wiki_links(urlList)\n",
        "    if len(url_wiki) >= 1:\n",
        "      keep = min(len(url_wiki), 3)\n",
        "      url_wiki = url_wiki[0:keep]\n",
        "      text = get_wiki_text(url_wiki)\n",
        "    else: \n",
        "      #Use the knowledge graph categories to find wikipedia url\n",
        "      print(\" No wiki urls: 1st pass\")\n",
        "      category = classify_input(knowledge_graph_df)\n",
        "      search_input = tailored_search(category, input)\n",
        "      print(f\"Searching for urls with input {search_input}\")\n",
        "      urlList = collect_urls(get_knowledge_graph_df(search_input))\n",
        "      url_wiki = get_wiki_links(urlList)\n",
        "      if len(url_wiki) >= 1:\n",
        "        keep = min(len(url_wiki), 3)\n",
        "        url_wiki = url_wiki[0:keep]\n",
        "        text = get_wiki_text(url_wiki)\n",
        "      else:\n",
        "        print(\" No wiki urls: 2nd pass\")\n",
        "        text = wiki_autosuggest(input)\n",
        "  \n",
        "  #Text Cleaning\n",
        "  text = re.sub(r\"\\'\", '', text) #Get rid of \\'\n",
        "  text = re.sub(r\"\\\\xa0...\", '', text) #Get rid of \\\\xa0...\n",
        "  text = re.sub(r\"\\\\n\", ' ', text) #Get rid of \\\\n\n",
        "  text = re.sub(r\"\\\\u200e\", ' ', text) #Get rid of \\\\u200e\n",
        "  text = re.sub(r\"[\\\"\\'\\“\\”\\[\\]\\)\\(\\•\\▽\\❖\\†]+\", '', text)\n",
        "  text = re.sub(r\"logo\", '', text)\n",
        "  text = re.sub(r\"[Vv]iew \\d+ more rows\", '', text) #Get rid of [Vv]iew \\d+ more rows\n",
        "  text = re.sub(r\"\\d+ hours ago\", '', text)\n",
        "  text = re.sub(r\"[-·—,.;:@#?!$+-]+\", ' ', text) \n",
        "  text = re.sub(r\"U S \", \"US \", text)\n",
        "\n",
        "  text = ' '.join(text.split()) #Single spacing\n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "def keyword_extract(text, ngram_size):\n",
        "  '''Extract keywords/phrases of ngram_size using YAKE'''\n",
        "  #Initialise extractor\n",
        "  kw_extractor = yake.KeywordExtractor()\n",
        "  language = \"en\"\n",
        "  max_ngram_size = ngram_size\n",
        "  deduplication_threshold = 0.3\n",
        "  numOfKeywords = 100\n",
        "  custom_kw_extractor = yake.KeywordExtractor(lan=language, \n",
        "                                              n=max_ngram_size, \n",
        "                                              dedupLim=deduplication_threshold, \n",
        "                                              top=numOfKeywords, features=None)\n",
        "  \n",
        "  #Run extractor on text and get out words/phrases\n",
        "  yake_output = custom_kw_extractor.extract_keywords(text)\n",
        "  words, scores = zip(*yake_output)\n",
        "  words = list(words)\n",
        "  scores = list(scores)\n",
        "  words = [re.sub(r\"[,.;@#?!$]+\", ' ', i) for i in words]\n",
        "  return(words,scores)\n",
        "\n"
      ],
      "metadata": {
        "id": "LW1rs9HGAWhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtering our keywords\n",
        "\n",
        "We check there are no repeats in our candidate keywords. We also want to select keywords which are proper nouns - these are the words we are mostly likely able to generate questions from in the next stage of this project."
      ],
      "metadata": {
        "id": "CqCZaQ2cAj_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def answer_keyword_compare(keywords_list, input_words):\n",
        "  '''Remove candidate keywords that contain input words'''\n",
        "\n",
        "  keywords_list = [x for x in keywords_list if not any(i in input_words for i in x.split())]\n",
        "  return keywords_list\n",
        "\n",
        "def remove_non_noun_full_keywords(keywords_list):\n",
        "  '''\n",
        "  Only retain keywords/keyphrases that are proper nouns.\n",
        "  '''\n",
        "  pos = nltk.pos_tag(keywords_list)\n",
        "  new_keyword_list = []\n",
        "  for ii in np.arange(0,len(pos),1):\n",
        "    if pos[ii][1]=='NNP':\n",
        "      new_keyword_list.append(pos[ii][0])\n",
        "    if pos[ii][1]=='NNPS':\n",
        "      new_keyword_list.append(pos[ii][0])\n",
        "  return new_keyword_list\n",
        "\n",
        "def select_keywords(words2):\n",
        "  '''\n",
        "  Selects the keywords/phrases to use for question generation. Ensures that \n",
        "  keyword phrases do not overlap each other.\n",
        "  '''\n",
        "  words3 = []\n",
        "  words3.append(words2[0])\n",
        "  del words2[0]\n",
        "  for i in range(len(words2)):\n",
        "    #If at any point, we only have 4 candidate keywords left, use them all\n",
        "    if len(words2) + len(words3) <= 4:\n",
        "      words3 = words3 + words2\n",
        "      break\n",
        "    test_words = words2[0].lower().split()\n",
        "    singles = [singularize(plural) for plural in test_words]\n",
        "    plurals1 = [pluralize(single) for single in singles]\n",
        "    plurals2 = [ending_pluralize(single) for single in singles]\n",
        "    plurals3 = [add_s_pluralize(single) for single in singles]\n",
        "    test_words = list(set(test_words + singles + plurals1 + plurals2 + plurals3))\n",
        "    previous_words = words3.copy()\n",
        "    previous_words = [word for phrase in previous_words for word in phrase.split()]\n",
        "    previous_words = [x.lower() for x in previous_words]\n",
        "    if len(test_words) + len(previous_words) == len(list(set(test_words + previous_words))):\n",
        "      words3.append(words2[0])\n",
        "    del words2[0]\n",
        "  return(words3)"
      ],
      "metadata": {
        "id": "MqrlzDa6AjLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final function\n",
        "\n",
        "The end user at the moment only needs to use this function and calls all the other functions created to return a keyword list. In the next stage, the output of this function will act as the input for the question generation stage."
      ],
      "metadata": {
        "id": "WS1iNcWgAy2P"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAwJd3wV8swk"
      },
      "source": [
        "def linkee_keywords(input):\n",
        "  \"\"\"\n",
        "  Main pipeline function which takes input and generates list of keywords.\n",
        "  \"\"\"\n",
        "  answer_list = tidy_input(input)\n",
        "\n",
        "  #Keyword extraction\n",
        "  text = find_text(input)\n",
        "  \n",
        "  #Potentially here combine 1-gram, 2-gram, 3-gram results\n",
        "  words_df = pd.DataFrame()\n",
        "  words = (keyword_extract(text, 2))[0] #+ (keyword_extract(text, 1))[0] + (keyword_extract(text, 3))[0]\n",
        "  scores = (keyword_extract(text, 2))[1] #+ (keyword_extract(text, 1))[1] + (keyword_extract(text, 3))[1]\n",
        "  words_df['words'] = words\n",
        "  words_df['scores'] = scores\n",
        "  words_df.sort_values(by=['scores'],ascending=False)\n",
        "  words_df = words_df[:100]\n",
        "\n",
        "  #Cleaning up returned keywords\n",
        "  words2 = answer_keyword_compare(words, answer_list)\n",
        "\n",
        "  words2 = remove_non_noun_full_keywords(words2)\n",
        "\n",
        "  final_keywords = select_keywords(words2)\n",
        "\n",
        "  return(final_keywords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a few pre-run examples of results. While some results appear to be quite vague/not useful, the only ones which will be kept on the final card will be those we can generate questions for. Therefore, the next stage of this project will likely bring the added benefit of improving our keyword list. \n"
      ],
      "metadata": {
        "id": "QVXNgFOsA_Su"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkMeeUqtNW2Y",
        "outputId": "a8e01753-b8c0-4365-e10d-d0f996a34ef4"
      },
      "source": [
        "%%time\n",
        "linkee_keywords('Apocalypse Now')[:6] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking at wiki page for: Apocalypse_Now\n",
            "Looking at wiki page for: Ride_of_the_Valkyries\n",
            "CPU times: user 27.3 s, sys: 542 ms, total: 27.9 s\n",
            "Wall time: 30.8 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Vietnam War',\n",
              " 'Ford Coppola',\n",
              " 'Colonel Kurtz',\n",
              " 'Marlon Brando',\n",
              " 'George Lucas',\n",
              " 'John Milius']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "linkee_keywords('Tom Hanks')[:6] "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRdCByTXbxsZ",
        "outputId": "995a02e3-3065-4df7-e7f7-af370bf3f61c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking at wiki page for: Tom_Hanks\n",
            "Looking at wiki page for: Forrest_Gump\n",
            "CPU times: user 24.7 s, sys: 471 ms, total: 25.2 s\n",
            "Wall time: 27.8 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Forrest Gump',\n",
              " 'Academy Award',\n",
              " 'American Film',\n",
              " 'Motion Picture',\n",
              " 'Golden Globe',\n",
              " 'South Carolina']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYpEH3Nx_ozJ",
        "outputId": "204530b4-02fd-48fb-bb50-1331ef885068"
      },
      "source": [
        "%%time\n",
        "linkee_keywords('Eddie Murphy')[:6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking at wiki page for: Eddie_Murphy\n",
            "CPU times: user 14.7 s, sys: 261 ms, total: 15 s\n",
            "Wall time: 16.8 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hills Cop',\n",
              " 'Night Live',\n",
              " 'Nutty Professor',\n",
              " 'Academy Award',\n",
              " 'Supporting Actor',\n",
              " 'Paramount Pictures']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "linkee_keywords('Allstate')[:6]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe6LFN07yQHk",
        "outputId": "bf78dc7f-64bd-4ab7-84de-03d723dfdf4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking at wiki page for: Allstate\n",
            "CPU times: user 18.5 s, sys: 278 ms, total: 18.8 s\n",
            "Wall time: 20.5 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sears Roebuck',\n",
              " 'Wrigley Field',\n",
              " 'Dennis Haysbert',\n",
              " 'Solutions Private',\n",
              " 'National General',\n",
              " 'Northbrook Illinois']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "linkee_keywords('Green Goblin')[:6]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX-cWzI2YY3a",
        "outputId": "3f1e12e8-821c-423b-9c64-ca0ae5b8ac12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking at wiki page for: Green_Goblin\n",
            "Looking at wiki page for: Harry_Osborn\n",
            "CPU times: user 22.7 s, sys: 436 ms, total: 23.1 s\n",
            "Wall time: 26.3 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Spider Man',\n",
              " 'Harry Osborn',\n",
              " 'American Son',\n",
              " 'Parker Industries',\n",
              " 'Gabriel Stacy',\n",
              " 'Formula Norman']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "linkee_keywords('Emmerdale')[:6]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "611D4lRCYeQd",
        "outputId": "4e36793d-72e4-41c3-e239-674e34fb9017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking at wiki page for: Emmerdale\n",
            "CPU times: user 15.3 s, sys: 213 ms, total: 15.6 s\n",
            "Wall time: 17.4 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Scottish Television',\n",
              " 'Tom King',\n",
              " 'ITV regions',\n",
              " 'Jack Sugden',\n",
              " 'Yorkshire Dales',\n",
              " 'Episodes originally']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    }
  ]
}